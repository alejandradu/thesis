2025-02-05 14:44:02,551	INFO worker.py:1841 -- Started a local Ray instance.
2025-02-05 14:44:06,625	INFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2025-02-05 14:44:06,846	INFO data_parallel_trainer.py:340 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.
[36m(TorchTrainer pid=874184)[0m GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.
[36m(RayTrainWorker pid=875429)[0m Setting up process group for: env:// [rank=0, world_size=8]
[36m(TorchTrainer pid=874184)[0m Started distributed worker processes: 
[36m(TorchTrainer pid=874184)[0m - (node_id=9141ad9496c39d40a3844392475e6830ba9725c4c6943e12a02c7185, ip=172.17.6.205, pid=875429) world_rank=0, local_rank=0, node_rank=0
[36m(TorchTrainer pid=874184)[0m - (node_id=9141ad9496c39d40a3844392475e6830ba9725c4c6943e12a02c7185, ip=172.17.6.205, pid=875427) world_rank=1, local_rank=1, node_rank=0
[36m(TorchTrainer pid=874184)[0m - (node_id=9141ad9496c39d40a3844392475e6830ba9725c4c6943e12a02c7185, ip=172.17.6.205, pid=875425) world_rank=2, local_rank=2, node_rank=0
[36m(TorchTrainer pid=874184)[0m - (node_id=9141ad9496c39d40a3844392475e6830ba9725c4c6943e12a02c7185, ip=172.17.6.205, pid=875440) world_rank=3, local_rank=3, node_rank=0
[36m(TorchTrainer pid=874184)[0m - (node_id=9141ad9496c39d40a3844392475e6830ba9725c4c6943e12a02c7185, ip=172.17.6.205, pid=875441) world_rank=4, local_rank=4, node_rank=0
[36m(TorchTrainer pid=874184)[0m - (node_id=9141ad9496c39d40a3844392475e6830ba9725c4c6943e12a02c7185, ip=172.17.6.205, pid=875428) world_rank=5, local_rank=5, node_rank=0
[36m(TorchTrainer pid=874184)[0m - (node_id=9141ad9496c39d40a3844392475e6830ba9725c4c6943e12a02c7185, ip=172.17.6.205, pid=875439) world_rank=6, local_rank=6, node_rank=0
[36m(TorchTrainer pid=874184)[0m - (node_id=9141ad9496c39d40a3844392475e6830ba9725c4c6943e12a02c7185, ip=172.17.6.205, pid=875426) world_rank=7, local_rank=7, node_rank=0
2025-02-05 14:44:22,696	ERROR tune_controller.py:1331 -- Trial task failed for trial TorchTrainer_8f89f_00000
Traceback (most recent call last):
  File "/home/ad2002/.conda/envs/neuralnets/lib/python3.12/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
             ^^^^^^^^^^^^^^^
  File "/home/ad2002/.conda/envs/neuralnets/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/ad2002/.conda/envs/neuralnets/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ad2002/.conda/envs/neuralnets/lib/python3.12/site-packages/ray/_private/worker.py", line 2772, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ad2002/.conda/envs/neuralnets/lib/python3.12/site-packages/ray/_private/worker.py", line 919, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(BlockingIOError): [36mray::_Inner.train()[39m (pid=874184, ip=172.17.6.205, actor_id=af1c03ccfdc446c12feafad201000000, repr=TorchTrainer)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ad2002/.conda/envs/neuralnets/lib/python3.12/site-packages/ray/tune/trainable/trainable.py", line 331, in train
    raise skipped from exception_cause(skipped)
  File "/home/ad2002/.conda/envs/neuralnets/lib/python3.12/site-packages/ray/train/_internal/utils.py", line 57, in check_for_failure
    ray.get(object_ref)
           ^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ray.exceptions.RayTaskError(BlockingIOError): [36mray::_RayTrainWorker__execute.get_next()[39m (pid=875427, ip=172.17.6.205, actor_id=09457f42bc1974913920c1c501000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x14c998a278f0>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ad2002/.conda/envs/neuralnets/lib/python3.12/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/home/ad2002/.conda/envs/neuralnets/lib/python3.12/site-packages/ray/train/_internal/utils.py", line 176, in discard_return_wrapper
    train_func(*args, **kwargs)
  File "/home/ad2002/thesis/scripts_to_run/train_cluster.py", line 124, in train_loop
    data_module.prepare_data()
  File "/home/ad2002/thesis/synthetic_datasets/datamodules/task_datamodule.py", line 106, in prepare_data
    with h5py.File(self.dpath, 'w') as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ad2002/.conda/envs/neuralnets/lib/python3.12/site-packages/h5py/_hl/files.py", line 561, in __init__
    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ad2002/.conda/envs/neuralnets/lib/python3.12/site-packages/h5py/_hl/files.py", line 241, in make_fid
    fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h5py/_objects.pyx", line 54, in h5py._objects.with_phil.wrapper
  File "h5py/_objects.pyx", line 55, in h5py._objects.with_phil.wrapper
  File "h5py/h5f.pyx", line 122, in h5py.h5f.create
BlockingIOError: [Errno 11] Unable to synchronously create file (unable to lock file, errno = 11, error message = 'Resource temporarily unavailable')
2025-02-05 14:44:22,711	INFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-05_14-44-00' in 0.0087s.
2025-02-05 14:44:22,713	ERROR tune.py:1037 -- Trials did not complete: [TorchTrainer_8f89f_00000]
