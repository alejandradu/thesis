
CondaError: Run 'conda init' before 'conda activate'

2025-01-31 17:29:41,908	INFO worker.py:1841 -- Started a local Ray instance.
2025-01-31 17:29:57,815	INFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
[36m(RayTrainWorker pid=2408069)[0m Setting up process group for: env:// [rank=0, world_size=4]
2025-01-31 17:30:15,747	ERROR tune_controller.py:1331 -- Trial task failed for trial TorchTrainer_e6d7f_00000
Traceback (most recent call last):
  File "/home/ad2002/.conda/envs/thesis/lib/python3.11/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
             ^^^^^^^^^^^^^^^
  File "/home/ad2002/.conda/envs/thesis/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/ad2002/.conda/envs/thesis/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ad2002/.conda/envs/thesis/lib/python3.11/site-packages/ray/_private/worker.py", line 2772, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ad2002/.conda/envs/thesis/lib/python3.11/site-packages/ray/_private/worker.py", line 919, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(RuntimeError): [36mray::_Inner.train()[39m (pid=2407879, ip=172.17.6.68, actor_id=405f61f7661f65a40e8355f101000000, repr=TorchTrainer)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ad2002/.conda/envs/thesis/lib/python3.11/site-packages/ray/tune/trainable/trainable.py", line 331, in train
    raise skipped from exception_cause(skipped)
  File "/home/ad2002/.conda/envs/thesis/lib/python3.11/site-packages/ray/air/_internal/util.py", line 107, in run
    self._ret = self._target(*self._args, **self._kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ad2002/.conda/envs/thesis/lib/python3.11/site-packages/ray/tune/trainable/function_trainable.py", line 44, in <lambda>
    training_func=lambda: self._trainable_func(self.config),
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ad2002/.conda/envs/thesis/lib/python3.11/site-packages/ray/train/base_trainer.py", line 799, in _trainable_func
    super()._trainable_func(self._merged_config)
  File "/home/ad2002/.conda/envs/thesis/lib/python3.11/site-packages/ray/tune/trainable/function_trainable.py", line 249, in _trainable_func
    output = fn()
             ^^^^
  File "/home/ad2002/.conda/envs/thesis/lib/python3.11/site-packages/ray/train/base_trainer.py", line 107, in _train_coordinator_fn
    trainer.training_loop()
  File "/home/ad2002/.conda/envs/thesis/lib/python3.11/site-packages/ray/train/data_parallel_trainer.py", line 459, in training_loop
    backend_executor.start()
  File "/home/ad2002/.conda/envs/thesis/lib/python3.11/site-packages/ray/train/_internal/backend_executor.py", line 210, in start
    self._backend.on_start(self.worker_group, self._backend_config)
  File "/home/ad2002/.conda/envs/thesis/lib/python3.11/site-packages/ray/train/torch/config.py", line 200, in on_start
    ray.get(setup_futures)
           ^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ray.exceptions.RayTaskError(RuntimeError): [36mray::_RayTrainWorker__execute._setup_torch_process_group()[39m (pid=2408069, ip=172.17.6.68, actor_id=75c76c8e0f95f646ac67b4cc01000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x14e79d0a4f10>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ad2002/.conda/envs/thesis/lib/python3.11/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/home/ad2002/.conda/envs/thesis/lib/python3.11/site-packages/ray/train/_internal/worker_group.py", line 30, in __execute
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ad2002/.conda/envs/thesis/lib/python3.11/site-packages/ray/train/torch/config.py", line 115, in _setup_torch_process_group
    dist.init_process_group(
  File "/home/ad2002/.conda/envs/thesis/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ad2002/.conda/envs/thesis/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 89, in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ad2002/.conda/envs/thesis/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 1312, in init_process_group
    default_pg, _ = _new_process_group_helper(
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ad2002/.conda/envs/thesis/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 1513, in _new_process_group_helper
    raise RuntimeError("Distributed package doesn't have NCCL built in")
RuntimeError: Distributed package doesn't have NCCL built in
2025-01-31 17:30:15,756	INFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-01-31_17-29-39' in 0.0037s.
2025-01-31 17:30:15,758	ERROR tune.py:1037 -- Trials did not complete: [TorchTrainer_e6d7f_00000]
[36m(RayTrainWorker pid=2408071)[0m *** SIGSEGV received at time=1738362615 on cpu 24 ***
[36m(RayTrainWorker pid=2408071)[0m PC: @     0x1462be47232e  (unknown)  ray::gcs::TaskInfoAccessor::AsyncAddTaskEventData()
[36m(RayTrainWorker pid=2408071)[0m     @     0x1462c016dd10       3392  (unknown)
[36m(RayTrainWorker pid=2408071)[0m     @     0x1462be355075       1392  ray::core::worker::TaskEventBufferImpl::FlushEvents()
[36m(RayTrainWorker pid=2408071)[0m     @     0x1462be2e103c       1488  ray::core::CoreWorker::Disconnect()
[36m(RayTrainWorker pid=2408071)[0m     @     0x1462be2e13ed       1152  ray::core::CoreWorker::ForceExit()
[36m(RayTrainWorker pid=2408071)[0m     @     0x1462be2e181f       1680  ray::core::CoreWorker::HandleKillActor()
[36m(RayTrainWorker pid=2408071)[0m     @     0x1462be2d8d24        192  ray::rpc::ServerCallImpl<>::HandleRequestImpl()
[36m(RayTrainWorker pid=2408071)[0m     @     0x1462be627758       1168  EventTracker::RecordExecution()
[36m(RayTrainWorker pid=2408071)[0m     @     0x1462be62272e         48  std::_Function_handler<>::_M_invoke()
[36m(RayTrainWorker pid=2408071)[0m     @     0x1462be622ba6        112  boost::asio::detail::completion_handler<>::do_complete()
[36m(RayTrainWorker pid=2408071)[0m     @     0x1462bebf82fb        128  boost::asio::detail::scheduler::do_run_one()
[36m(RayTrainWorker pid=2408071)[0m     @     0x1462bebf9c79        288  boost::asio::detail::scheduler::run()
[36m(RayTrainWorker pid=2408071)[0m     @     0x1462bebfa382         96  boost::asio::io_context::run()
[36m(RayTrainWorker pid=2408071)[0m     @     0x1462be21e1b1       1280  ray::core::CoreWorker::RunIOService()
[36m(RayTrainWorker pid=2408071)[0m     @     0x1462be6d8ec0         64  thread_proxy
[36m(RayTrainWorker pid=2408071)[0m     @     0x1462c01631ca  (unknown)  start_thread
[36m(RayTrainWorker pid=2408071)[0m [2025-01-31 17:30:16,061 E 2408071 2408283] logging.cc:460: *** SIGSEGV received at time=1738362616 on cpu 24 ***
[36m(RayTrainWorker pid=2408071)[0m [2025-01-31 17:30:16,061 E 2408071 2408283] logging.cc:460: PC: @     0x1462be47232e  (unknown)  ray::gcs::TaskInfoAccessor::AsyncAddTaskEventData()
[36m(RayTrainWorker pid=2408071)[0m [2025-01-31 17:30:16,061 E 2408071 2408283] logging.cc:460:     @     0x1462c016dd10       3392  (unknown)
[36m(RayTrainWorker pid=2408071)[0m [2025-01-31 17:30:16,061 E 2408071 2408283] logging.cc:460:     @     0x1462be355075       1392  ray::core::worker::TaskEventBufferImpl::FlushEvents()
[36m(RayTrainWorker pid=2408071)[0m [2025-01-31 17:30:16,061 E 2408071 2408283] logging.cc:460:     @     0x1462be2e103c       1488  ray::core::CoreWorker::Disconnect()
[36m(RayTrainWorker pid=2408071)[0m [2025-01-31 17:30:16,061 E 2408071 2408283] logging.cc:460:     @     0x1462be2e13ed       1152  ray::core::CoreWorker::ForceExit()
[36m(RayTrainWorker pid=2408071)[0m [2025-01-31 17:30:16,061 E 2408071 2408283] logging.cc:460:     @     0x1462be2e181f       1680  ray::core::CoreWorker::HandleKillActor()
[36m(RayTrainWorker pid=2408071)[0m [2025-01-31 17:30:16,061 E 2408071 2408283] logging.cc:460:     @     0x1462be2d8d24        192  ray::rpc::ServerCallImpl<>::HandleRequestImpl()
[36m(RayTrainWorker pid=2408071)[0m [2025-01-31 17:30:16,061 E 2408071 2408283] logging.cc:460:     @     0x1462be627758       1168  EventTracker::RecordExecution()
[36m(RayTrainWorker pid=2408071)[0m [2025-01-31 17:30:16,061 E 2408071 2408283] logging.cc:460:     @     0x1462be62272e         48  std::_Function_handler<>::_M_invoke()
[36m(RayTrainWorker pid=2408071)[0m [2025-01-31 17:30:16,061 E 2408071 2408283] logging.cc:460:     @     0x1462be622ba6        112  boost::asio::detail::completion_handler<>::do_complete()
[36m(RayTrainWorker pid=2408071)[0m [2025-01-31 17:30:16,061 E 2408071 2408283] logging.cc:460:     @     0x1462bebf82fb        128  boost::asio::detail::scheduler::do_run_one()
[36m(RayTrainWorker pid=2408071)[0m [2025-01-31 17:30:16,061 E 2408071 2408283] logging.cc:460:     @     0x1462bebf9c79        288  boost::asio::detail::scheduler::run()
[36m(RayTrainWorker pid=2408071)[0m [2025-01-31 17:30:16,061 E 2408071 2408283] logging.cc:460:     @     0x1462bebfa382         96  boost::asio::io_context::run()
[36m(RayTrainWorker pid=2408071)[0m [2025-01-31 17:30:16,061 E 2408071 2408283] logging.cc:460:     @     0x1462be21e1b1       1280  ray::core::CoreWorker::RunIOService()
[36m(RayTrainWorker pid=2408071)[0m [2025-01-31 17:30:16,061 E 2408071 2408283] logging.cc:460:     @     0x1462be6d8ec0         64  thread_proxy
[36m(RayTrainWorker pid=2408071)[0m [2025-01-31 17:30:16,061 E 2408071 2408283] logging.cc:460:     @     0x1462c01631ca  (unknown)  start_thread
[36m(RayTrainWorker pid=2408071)[0m Fatal Python error: Segmentation fault
[36m(RayTrainWorker pid=2408071)[0m 
[36m(RayTrainWorker pid=2408071)[0m 
[36m(RayTrainWorker pid=2408071)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, zstandard.backend_c, ray._raylet, mkl._mklinit, mkl._py_mkl_service, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, pyarrow.lib, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pyarrow._compute, pandas._libs.ops, pandas._libs.hashing, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.internals, pandas._libs.indexing, pandas._libs.index, pandas._libs.writers, pandas._libs.join, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, pyarrow._parquet, pyarrow._json, torch._C, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, gmpy2.gmpy2 (total: 81)
