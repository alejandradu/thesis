
CondaError: Run 'conda init' before 'conda activate'

2025-02-07 04:03:20,970	INFO worker.py:1841 -- Started a local Ray instance.
2025-02-07 04:03:23,354	INFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2025-02-07 04:03:23,372	INFO data_parallel_trainer.py:340 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.
2025-02-07 04:03:23,374	INFO data_parallel_trainer.py:340 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.
2025-02-07 04:03:23,376	INFO data_parallel_trainer.py:340 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.
2025-02-07 04:03:23,378	INFO data_parallel_trainer.py:340 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.
2025-02-07 04:03:23,380	INFO data_parallel_trainer.py:340 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.
2025-02-07 04:03:23,382	INFO data_parallel_trainer.py:340 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.
2025-02-07 04:03:23,384	INFO data_parallel_trainer.py:340 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.
2025-02-07 04:03:23,386	INFO data_parallel_trainer.py:340 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.
[36m(TorchTrainer pid=1348037)[0m GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.
[36m(RayTrainWorker pid=1349241)[0m Setting up process group for: env:// [rank=0, world_size=8]
[36m(TorchTrainer pid=1348040)[0m GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.[32m [repeated 4x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(TorchTrainer pid=1348034)[0m Started distributed worker processes: 
[36m(TorchTrainer pid=1348034)[0m - (node_id=049cb75edba8a3001da8f4da1e35abcbde72dd6c7f1fae22bc2fcd05, ip=172.17.6.198, pid=1349249) world_rank=0, local_rank=0, node_rank=0
[36m(TorchTrainer pid=1348034)[0m - (node_id=049cb75edba8a3001da8f4da1e35abcbde72dd6c7f1fae22bc2fcd05, ip=172.17.6.198, pid=1349326) world_rank=1, local_rank=1, node_rank=0
[36m(TorchTrainer pid=1348034)[0m - (node_id=049cb75edba8a3001da8f4da1e35abcbde72dd6c7f1fae22bc2fcd05, ip=172.17.6.198, pid=1349314) world_rank=2, local_rank=2, node_rank=0
[36m(TorchTrainer pid=1348034)[0m - (node_id=049cb75edba8a3001da8f4da1e35abcbde72dd6c7f1fae22bc2fcd05, ip=172.17.6.198, pid=1349244) world_rank=3, local_rank=3, node_rank=0
[36m(TorchTrainer pid=1348034)[0m - (node_id=049cb75edba8a3001da8f4da1e35abcbde72dd6c7f1fae22bc2fcd05, ip=172.17.6.198, pid=1349321) world_rank=4, local_rank=4, node_rank=0
[36m(TorchTrainer pid=1348034)[0m - (node_id=049cb75edba8a3001da8f4da1e35abcbde72dd6c7f1fae22bc2fcd05, ip=172.17.6.198, pid=1349256) world_rank=5, local_rank=5, node_rank=0
[36m(TorchTrainer pid=1348034)[0m - (node_id=049cb75edba8a3001da8f4da1e35abcbde72dd6c7f1fae22bc2fcd05, ip=172.17.6.198, pid=1349254) world_rank=6, local_rank=6, node_rank=0
[36m(TorchTrainer pid=1348034)[0m - (node_id=049cb75edba8a3001da8f4da1e35abcbde72dd6c7f1fae22bc2fcd05, ip=172.17.6.198, pid=1349317) world_rank=7, local_rank=7, node_rank=0
[36m(RayTrainWorker pid=1349249)[0m Setting up process group for: env:// [rank=0, world_size=8]
[36m(RayTrainWorker pid=1349245)[0m Setting up process group for: env:// [rank=0, world_size=8]
[36m(RayTrainWorker pid=1349249)[0m GPU available: False, used: False
[36m(RayTrainWorker pid=1349249)[0m TPU available: False, using: 0 TPU cores
[36m(RayTrainWorker pid=1349249)[0m HPU available: False, using: 0 HPUs
[36m(RayTrainWorker pid=1349249)[0m /home/ad2002/.conda/envs/neuralnets/lib/python3.12/site-packages/lightning/pytorch/loops/utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[36m(RayTrainWorker pid=1349249)[0m 
[36m(RayTrainWorker pid=1349249)[0m   | Name  | Type  | Params | Mode 
[36m(RayTrainWorker pid=1349249)[0m ----------------------------------------
[36m(RayTrainWorker pid=1349249)[0m 0 | model | frRNN | 175    | train
[36m(RayTrainWorker pid=1349249)[0m ----------------------------------------
[36m(RayTrainWorker pid=1349249)[0m 105       Trainable params
[36m(RayTrainWorker pid=1349249)[0m 70        Non-trainable params
[36m(RayTrainWorker pid=1349249)[0m 175       Total params
[36m(RayTrainWorker pid=1349249)[0m 0.001     Total estimated model params size (MB)
[36m(RayTrainWorker pid=1349249)[0m 1         Modules in train mode
[36m(RayTrainWorker pid=1349249)[0m 0         Modules in eval mode
[36m(RayTrainWorker pid=1349245)[0m 
[36m(RayTrainWorker pid=1349241)[0m 
[36m(TorchTrainer pid=1348040)[0m Started distributed worker processes: [32m [repeated 2x across cluster][0m
[36m(TorchTrainer pid=1348040)[0m - (node_id=049cb75edba8a3001da8f4da1e35abcbde72dd6c7f1fae22bc2fcd05, ip=172.17.6.198, pid=1349250) world_rank=7, local_rank=7, node_rank=0[32m [repeated 16x across cluster][0m
[36m(RayTrainWorker pid=1349246)[0m Setting up process group for: env:// [rank=0, world_size=8][32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1349240)[0m 
[36m(RayTrainWorker pid=1349246)[0m 
[36m(RayTrainWorker pid=1349249)[0m /home/ad2002/.conda/envs/neuralnets/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[36m(RayTrainWorker pid=1349240)[0m GPU available: False, used: False[32m [repeated 4x across cluster][0m
[36m(RayTrainWorker pid=1349240)[0m TPU available: False, using: 0 TPU cores[32m [repeated 4x across cluster][0m
[36m(RayTrainWorker pid=1349240)[0m HPU available: False, using: 0 HPUs[32m [repeated 4x across cluster][0m
[36m(RayTrainWorker pid=1349240)[0m /home/ad2002/.conda/envs/neuralnets/lib/python3.12/site-packages/lightning/pytorch/loops/utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.[32m [repeated 4x across cluster][0m
[36m(RayTrainWorker pid=1349246)[0m   | Name  | Type  | Params | Mode [32m [repeated 4x across cluster][0m
[36m(RayTrainWorker pid=1349246)[0m ----------------------------------------[32m [repeated 8x across cluster][0m
[36m(RayTrainWorker pid=1349246)[0m 0 | model | frRNN | 175    | train[32m [repeated 4x across cluster][0m
[36m(RayTrainWorker pid=1349246)[0m 105       Trainable params[32m [repeated 4x across cluster][0m
[36m(RayTrainWorker pid=1349246)[0m 70        Non-trainable params[32m [repeated 4x across cluster][0m
[36m(RayTrainWorker pid=1349246)[0m 175       Total params[32m [repeated 4x across cluster][0m
[36m(RayTrainWorker pid=1349246)[0m 0.001     Total estimated model params size (MB)[32m [repeated 4x across cluster][0m
[36m(RayTrainWorker pid=1349246)[0m 1         Modules in train mode[32m [repeated 4x across cluster][0m
[36m(RayTrainWorker pid=1349246)[0m 0         Modules in eval mode[32m [repeated 4x across cluster][0m
[36m(TorchTrainer pid=1348039)[0m Started distributed worker processes: [32m [repeated 2x across cluster][0m
[36m(TorchTrainer pid=1348039)[0m - (node_id=049cb75edba8a3001da8f4da1e35abcbde72dd6c7f1fae22bc2fcd05, ip=172.17.6.198, pid=1349316) world_rank=7, local_rank=7, node_rank=0[32m [repeated 16x across cluster][0m
[36m(RayTrainWorker pid=1349249)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00002_2_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000000)
[36m(RayTrainWorker pid=1349240)[0m /home/ad2002/.conda/envs/neuralnets/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.[32m [repeated 4x across cluster][0m
[36m(RayTrainWorker pid=1349249)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00002_2_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000001)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349249)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00002_2_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000002)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349241)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00004_4_lr=0.0010,noise_std=0.0000,weight_decay=0.0000_2025-02-07_04-03-23/checkpoint_000003)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349246)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00003_3_lr=0.0001,noise_std=0.1000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000004)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349246)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00003_3_lr=0.0001,noise_std=0.1000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000005)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349246)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00003_3_lr=0.0001,noise_std=0.1000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000006)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349246)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00003_3_lr=0.0001,noise_std=0.1000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000007)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349246)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00003_3_lr=0.0001,noise_std=0.1000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000008)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349246)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00003_3_lr=0.0001,noise_std=0.1000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000009)[32m [repeated 32x across cluster][0m
[36m(RayTrainWorker pid=1349246)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00003_3_lr=0.0001,noise_std=0.1000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000010)[32m [repeated 32x across cluster][0m
[36m(RayTrainWorker pid=1349245)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00001_1_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000010)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349245)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00001_1_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000011)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349245)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00001_1_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000012)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349245)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00001_1_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000013)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349240)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00000_0_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000014)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349245)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00001_1_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000015)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349245)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00001_1_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000016)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349325)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00000_0_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000017)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349245)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00001_1_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000018)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349245)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00001_1_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000019)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349241)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00004_4_lr=0.0010,noise_std=0.0000,weight_decay=0.0000_2025-02-07_04-03-23/checkpoint_000020)[32m [repeated 32x across cluster][0m
[36m(RayTrainWorker pid=1349241)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00004_4_lr=0.0010,noise_std=0.0000,weight_decay=0.0000_2025-02-07_04-03-23/checkpoint_000021)[32m [repeated 32x across cluster][0m
[36m(RayTrainWorker pid=1349241)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00004_4_lr=0.0010,noise_std=0.0000,weight_decay=0.0000_2025-02-07_04-03-23/checkpoint_000022)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349245)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00001_1_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000023)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349249)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00002_2_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000025)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349240)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00000_0_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000025)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349240)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00000_0_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000026)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349240)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00000_0_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000027)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349240)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00000_0_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000028)[32m [repeated 36x across cluster][0m
[36m(RayTrainWorker pid=1349241)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00004_4_lr=0.0010,noise_std=0.0000,weight_decay=0.0000_2025-02-07_04-03-23/checkpoint_000028)[32m [repeated 20x across cluster][0m
[36m(RayTrainWorker pid=1349249)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00002_2_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000030)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349246)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00003_3_lr=0.0001,noise_std=0.1000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000031)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349246)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00003_3_lr=0.0001,noise_std=0.1000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000032)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349246)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00003_3_lr=0.0001,noise_std=0.1000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000033)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349246)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00003_3_lr=0.0001,noise_std=0.1000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000034)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349259)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00003_3_lr=0.0001,noise_std=0.1000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000035)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349246)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00003_3_lr=0.0001,noise_std=0.1000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000036)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349246)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00003_3_lr=0.0001,noise_std=0.1000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000037)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349249)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00002_2_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000037)[32m [repeated 32x across cluster][0m
[36m(RayTrainWorker pid=1349245)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00001_1_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000037)[32m [repeated 24x across cluster][0m
[36m(RayTrainWorker pid=1349245)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00001_1_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000038)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349245)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00001_1_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000039)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349245)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00001_1_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000040)[32m [repeated 8x across cluster][0m
[36m(RayTrainWorker pid=1349245)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00001_1_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000041)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349245)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00001_1_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000042)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349245)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00001_1_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000043)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349249)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00002_2_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000044)[32m [repeated 48x across cluster][0m
[36m(RayTrainWorker pid=1349249)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00002_2_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000045)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349246)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00003_3_lr=0.0001,noise_std=0.1000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000046)[32m [repeated 24x across cluster][0m
[36m(RayTrainWorker pid=1349240)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00000_0_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000047)[32m [repeated 24x across cluster][0m
[36m(RayTrainWorker pid=1349240)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00000_0_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000048)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349240)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00000_0_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000049)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349249)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00002_2_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000049)[32m [repeated 16x across cluster][0m
[36m(RayTrainWorker pid=1349246)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00003_3_lr=0.0001,noise_std=0.1000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000050)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349249)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00002_2_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000051)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349249)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00002_2_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000052)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349249)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00002_2_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000053)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349249)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00002_2_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000054)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349249)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00002_2_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000055)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349246)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00003_3_lr=0.0001,noise_std=0.1000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000055)[32m [repeated 16x across cluster][0m
[36m(RayTrainWorker pid=1349246)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00003_3_lr=0.0001,noise_std=0.1000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000056)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349246)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00003_3_lr=0.0001,noise_std=0.1000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000057)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349246)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00003_3_lr=0.0001,noise_std=0.1000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000058)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349246)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00003_3_lr=0.0001,noise_std=0.1000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000059)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349240)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00000_0_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000062)[32m [repeated 48x across cluster][0m
[36m(RayTrainWorker pid=1349240)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00000_0_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000063)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349240)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00000_0_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000064)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349240)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00000_0_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000065)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349240)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00000_0_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000066)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349240)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00000_0_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000067)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349240)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00000_0_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000068)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349245)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00001_1_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000067)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349240)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00000_0_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000070)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349240)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00000_0_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000071)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349240)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00000_0_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000072)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349240)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00000_0_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000073)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349245)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00001_1_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000072)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349241)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00004_4_lr=0.0010,noise_std=0.0000,weight_decay=0.0000_2025-02-07_04-03-23/checkpoint_000072)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349241)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00004_4_lr=0.0010,noise_std=0.0000,weight_decay=0.0000_2025-02-07_04-03-23/checkpoint_000073)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349240)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00000_0_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000077)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349245)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00001_1_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000076)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349245)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00001_1_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000077)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349245)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00001_1_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000078)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349245)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00001_1_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000079)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349241)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00004_4_lr=0.0010,noise_std=0.0000,weight_decay=0.0000_2025-02-07_04-03-23/checkpoint_000078)[32m [repeated 8x across cluster][0m
[36m(RayTrainWorker pid=1349249)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00002_2_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000081)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349249)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00002_2_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000082)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349325)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00000_0_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000084)[32m [repeated 48x across cluster][0m
[36m(RayTrainWorker pid=1349241)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00004_4_lr=0.0010,noise_std=0.0000,weight_decay=0.0000_2025-02-07_04-03-23/checkpoint_000082)[32m [repeated 48x across cluster][0m
[36m(RayTrainWorker pid=1349243)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00001_1_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000085)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349241)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00004_4_lr=0.0010,noise_std=0.0000,weight_decay=0.0000_2025-02-07_04-03-23/checkpoint_000084)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349243)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00001_1_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000087)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349249)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00002_2_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000089)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349249)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00002_2_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000090)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349249)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00002_2_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000091)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349241)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00004_4_lr=0.0010,noise_std=0.0000,weight_decay=0.0000_2025-02-07_04-03-23/checkpoint_000089)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349241)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00004_4_lr=0.0010,noise_std=0.0000,weight_decay=0.0000_2025-02-07_04-03-23/checkpoint_000090)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349328)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00004_4_lr=0.0010,noise_std=0.0000,weight_decay=0.0000_2025-02-07_04-03-23/checkpoint_000091)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349245)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00001_1_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000094)[32m [repeated 48x across cluster][0m
[36m(RayTrainWorker pid=1349245)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00001_1_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000095)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349245)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00001_1_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000096)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349245)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00001_1_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000097)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1349240)[0m *** SIGSEGV received at time=1738919752 on cpu 5 ***
[36m(RayTrainWorker pid=1349240)[0m PC: @     0x1527728e12fe  (unknown)  ray::gcs::TaskInfoAccessor::AsyncAddTaskEventData()
[36m(RayTrainWorker pid=1349240)[0m     @     0x1527745d6d10       3648  (unknown)
[36m(RayTrainWorker pid=1349240)[0m     @     0x1527727c4045       1392  ray::core::worker::TaskEventBufferImpl::FlushEvents()
[36m(RayTrainWorker pid=1349240)[0m     @     0x15277275000c       1488  ray::core::CoreWorker::Disconnect()
[36m(RayTrainWorker pid=1349240)[0m     @     0x1527727503bd       1152  ray::core::CoreWorker::ForceExit()
[36m(RayTrainWorker pid=1349240)[0m     @     0x1527727507ef       1680  ray::core::CoreWorker::HandleKillActor()
[36m(RayTrainWorker pid=1349240)[0m     @     0x152772747cf4        192  ray::rpc::ServerCallImpl<>::HandleRequestImpl()
[36m(RayTrainWorker pid=1349240)[0m     @     0x152772a96728       1168  EventTracker::RecordExecution()
[36m(RayTrainWorker pid=1349240)[0m     @     0x152772a916fe         48  std::_Function_handler<>::_M_invoke()
[36m(RayTrainWorker pid=1349240)[0m     @     0x152772a91b76        112  boost::asio::detail::completion_handler<>::do_complete()
[36m(RayTrainWorker pid=1349240)[0m     @     0x1527730672bb        128  boost::asio::detail::scheduler::do_run_one()
[36m(RayTrainWorker pid=1349240)[0m     @     0x152773068c39        288  boost::asio::detail::scheduler::run()
[36m(RayTrainWorker pid=1349240)[0m     @     0x152773069342         96  boost::asio::io_context::run()
[36m(RayTrainWorker pid=1349240)[0m     @     0x15277268d101       1280  ray::core::CoreWorker::RunIOService()
[36m(RayTrainWorker pid=1349240)[0m     @     0x152772b47e90         64  thread_proxy
[36m(RayTrainWorker pid=1349240)[0m     @     0x1527745cc1ca  (unknown)  start_thread
[36m(RayTrainWorker pid=1349240)[0m [2025-02-07 04:15:52,775 E 1349240 1350538] logging.cc:460: *** SIGSEGV received at time=1738919752 on cpu 5 ***
[36m(RayTrainWorker pid=1349240)[0m [2025-02-07 04:15:52,775 E 1349240 1350538] logging.cc:460: PC: @     0x1527728e12fe  (unknown)  ray::gcs::TaskInfoAccessor::AsyncAddTaskEventData()
[36m(RayTrainWorker pid=1349240)[0m [2025-02-07 04:15:52,775 E 1349240 1350538] logging.cc:460:     @     0x1527745d6d10       3648  (unknown)
[36m(RayTrainWorker pid=1349240)[0m [2025-02-07 04:15:52,775 E 1349240 1350538] logging.cc:460:     @     0x1527727c4045       1392  ray::core::worker::TaskEventBufferImpl::FlushEvents()
[36m(RayTrainWorker pid=1349240)[0m [2025-02-07 04:15:52,775 E 1349240 1350538] logging.cc:460:     @     0x15277275000c       1488  ray::core::CoreWorker::Disconnect()
[36m(RayTrainWorker pid=1349240)[0m [2025-02-07 04:15:52,775 E 1349240 1350538] logging.cc:460:     @     0x1527727503bd       1152  ray::core::CoreWorker::ForceExit()
[36m(RayTrainWorker pid=1349240)[0m [2025-02-07 04:15:52,775 E 1349240 1350538] logging.cc:460:     @     0x1527727507ef       1680  ray::core::CoreWorker::HandleKillActor()
[36m(RayTrainWorker pid=1349240)[0m [2025-02-07 04:15:52,775 E 1349240 1350538] logging.cc:460:     @     0x152772747cf4        192  ray::rpc::ServerCallImpl<>::HandleRequestImpl()
[36m(RayTrainWorker pid=1349240)[0m [2025-02-07 04:15:52,775 E 1349240 1350538] logging.cc:460:     @     0x152772a96728       1168  EventTracker::RecordExecution()
[36m(RayTrainWorker pid=1349240)[0m [2025-02-07 04:15:52,775 E 1349240 1350538] logging.cc:460:     @     0x152772a916fe         48  std::_Function_handler<>::_M_invoke()
[36m(RayTrainWorker pid=1349240)[0m [2025-02-07 04:15:52,775 E 1349240 1350538] logging.cc:460:     @     0x152772a91b76        112  boost::asio::detail::completion_handler<>::do_complete()
[36m(RayTrainWorker pid=1349240)[0m [2025-02-07 04:15:52,775 E 1349240 1350538] logging.cc:460:     @     0x1527730672bb        128  boost::asio::detail::scheduler::do_run_one()
[36m(RayTrainWorker pid=1349240)[0m [2025-02-07 04:15:52,775 E 1349240 1350538] logging.cc:460:     @     0x152773068c39        288  boost::asio::detail::scheduler::run()
[36m(RayTrainWorker pid=1349240)[0m [2025-02-07 04:15:52,775 E 1349240 1350538] logging.cc:460:     @     0x152773069342         96  boost::asio::io_context::run()
[36m(RayTrainWorker pid=1349240)[0m [2025-02-07 04:15:52,775 E 1349240 1350538] logging.cc:460:     @     0x15277268d101       1280  ray::core::CoreWorker::RunIOService()
[36m(RayTrainWorker pid=1349240)[0m [2025-02-07 04:15:52,775 E 1349240 1350538] logging.cc:460:     @     0x152772b47e90         64  thread_proxy
[36m(RayTrainWorker pid=1349240)[0m [2025-02-07 04:15:52,775 E 1349240 1350538] logging.cc:460:     @     0x1527745cc1ca  (unknown)  start_thread
[36m(RayTrainWorker pid=1349240)[0m Fatal Python error: Segmentation fault
[36m(RayTrainWorker pid=1349240)[0m 
[36m(RayTrainWorker pid=1349240)[0m 
[36m(RayTrainWorker pid=1349240)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, ray._raylet, mkl._mklinit, mkl._py_mkl_service, numpy._core._multiarray_umath, numpy._core._multiarray_tests, numpy.linalg._umath_linalg, pyarrow.lib, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pyarrow._compute, pandas._libs.ops, pandas._libs.hashing, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.internals, pandas._libs.indexing, pandas._libs.index, pandas._libs.writers, pandas._libs.join, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, pyarrow._parquet, pyarrow._json, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, gmpy2.gmpy2, scipy._lib._ccallback_c, scipy.signal._sigtools, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.special._ufuncs_cxx, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.special._ellip_harm_2, scipy._lib._uarray._uarray, scipy.signal._max_len_seq_inner, scipy.signal._upfirdn_apply, scipy.signal._spline, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._distance_wrap, scipy.spatial._hausdorff, scipy.spatial.transform._rotation, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._cobyla, scipy.optimize._slsqp, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy.optimize._cython_nnls, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.interpolate._bspl, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, scipy.signal._sosfilt, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.special.cython_special, scipy.stats._stats, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.stats._mvn, scipy.stats._rcont.rcont, scipy.signal._peak_finding_utils, PIL._imaging, kiwisolver._cext, h5py._errors, h5py.defs, h5py._objects, h5py.h5, h5py.utils, h5py.h5t, h5py.h5s, h5py.h5ac, h5py.h5p, h5py.h5r, h5py._proxy, h5py._conv, h5py.h5z, h5py.h5a, h5py.h5d, h5py.h5ds, h5py.h5g, h5py.h5i, h5py.h5o, h5py.h5f, h5py.h5fd, h5py.h5pl, h5py.h5l, h5py._selector, sklearn.__check_build._check_build, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, sklearn.utils._random (total: 220)
[36m(RayTrainWorker pid=1349320)[0m 
[36m(RayTrainWorker pid=1349320)[0m 
[36m(TorchTrainer pid=1709398)[0m GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.
[36m(RayTrainWorker pid=1349250)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00004_4_lr=0.0010,noise_std=0.0000,weight_decay=0.0000_2025-02-07_04-03-23/checkpoint_000096)[32m [repeated 39x across cluster][0m
[36m(RayTrainWorker pid=1349320)[0m *** SIGSEGV received at time=1738919753 on cpu 7 ***
[36m(RayTrainWorker pid=1349320)[0m PC: @     0x14ed42ff72fe  (unknown)  ray::gcs::TaskInfoAccessor::AsyncAddTaskEventData()
[36m(RayTrainWorker pid=1349320)[0m     @     0x14ed44cecd10       3648  (unknown)
[36m(RayTrainWorker pid=1349320)[0m     @     0x14ed42eda045       1392  ray::core::worker::TaskEventBufferImpl::FlushEvents()
[36m(RayTrainWorker pid=1349320)[0m     @     0x14ed42e6600c       1488  ray::core::CoreWorker::Disconnect()
[36m(RayTrainWorker pid=1349320)[0m     @     0x14ed42e663bd       1152  ray::core::CoreWorker::ForceExit()
[36m(RayTrainWorker pid=1349320)[0m     @     0x14ed42e667ef       1680  ray::core::CoreWorker::HandleKillActor()
[36m(RayTrainWorker pid=1349320)[0m     @     0x14ed42e5dcf4        192  ray::rpc::ServerCallImpl<>::HandleRequestImpl()
[36m(RayTrainWorker pid=1349320)[0m     @     0x14ed431ac728       1168  EventTracker::RecordExecution()
[36m(RayTrainWorker pid=1349320)[0m     @     0x14ed431a76fe         48  std::_Function_handler<>::_M_invoke()
[36m(RayTrainWorker pid=1349320)[0m     @     0x14ed431a7b76        112  boost::asio::detail::completion_handler<>::do_complete()
[36m(RayTrainWorker pid=1349320)[0m     @     0x14ed4377d2bb        128  boost::asio::detail::scheduler::do_run_one()
[36m(RayTrainWorker pid=1349320)[0m     @     0x14ed4377ec39        288  boost::asio::detail::scheduler::run()
[36m(RayTrainWorker pid=1349320)[0m     @     0x14ed4377f342         96  boost::asio::io_context::run()
[36m(RayTrainWorker pid=1349320)[0m     @     0x14ed42da3101       1280  ray::core::CoreWorker::RunIOService()
[36m(RayTrainWorker pid=1349320)[0m     @     0x14ed4325de90         64  thread_proxy
[36m(RayTrainWorker pid=1349320)[0m     @     0x14ed44ce21ca  (unknown)  start_thread
[36m(RayTrainWorker pid=1349320)[0m [2025-02-07 04:15:53,437 E 1349320 1353719] logging.cc:460: *** SIGSEGV received at time=1738919753 on cpu 7 ***
[36m(RayTrainWorker pid=1349320)[0m [2025-02-07 04:15:53,437 E 1349320 1353719] logging.cc:460: PC: @     0x14ed42ff72fe  (unknown)  ray::gcs::TaskInfoAccessor::AsyncAddTaskEventData()
[36m(RayTrainWorker pid=1349320)[0m [2025-02-07 04:15:53,437 E 1349320 1353719] logging.cc:460:     @     0x14ed44cecd10       3648  (unknown)
[36m(RayTrainWorker pid=1349320)[0m [2025-02-07 04:15:53,437 E 1349320 1353719] logging.cc:460:     @     0x14ed42eda045       1392  ray::core::worker::TaskEventBufferImpl::FlushEvents()
[36m(RayTrainWorker pid=1349320)[0m [2025-02-07 04:15:53,437 E 1349320 1353719] logging.cc:460:     @     0x14ed42e6600c       1488  ray::core::CoreWorker::Disconnect()
[36m(RayTrainWorker pid=1349320)[0m [2025-02-07 04:15:53,437 E 1349320 1353719] logging.cc:460:     @     0x14ed42e663bd       1152  ray::core::CoreWorker::ForceExit()
[36m(RayTrainWorker pid=1349320)[0m [2025-02-07 04:15:53,437 E 1349320 1353719] logging.cc:460:     @     0x14ed42e667ef       1680  ray::core::CoreWorker::HandleKillActor()
[36m(RayTrainWorker pid=1349320)[0m [2025-02-07 04:15:53,437 E 1349320 1353719] logging.cc:460:     @     0x14ed42e5dcf4        192  ray::rpc::ServerCallImpl<>::HandleRequestImpl()
[36m(RayTrainWorker pid=1349320)[0m [2025-02-07 04:15:53,437 E 1349320 1353719] logging.cc:460:     @     0x14ed431ac728       1168  EventTracker::RecordExecution()
[36m(RayTrainWorker pid=1349320)[0m [2025-02-07 04:15:53,437 E 1349320 1353719] logging.cc:460:     @     0x14ed431a76fe         48  std::_Function_handler<>::_M_invoke()
[36m(RayTrainWorker pid=1349320)[0m [2025-02-07 04:15:53,437 E 1349320 1353719] logging.cc:460:     @     0x14ed431a7b76        112  boost::asio::detail::completion_handler<>::do_complete()
[36m(RayTrainWorker pid=1349320)[0m [2025-02-07 04:15:53,437 E 1349320 1353719] logging.cc:460:     @     0x14ed4377d2bb        128  boost::asio::detail::scheduler::do_run_one()
[36m(RayTrainWorker pid=1349320)[0m [2025-02-07 04:15:53,437 E 1349320 1353719] logging.cc:460:     @     0x14ed4377ec39        288  boost::asio::detail::scheduler::run()
[36m(RayTrainWorker pid=1349320)[0m [2025-02-07 04:15:53,437 E 1349320 1353719] logging.cc:460:     @     0x14ed4377f342         96  boost::asio::io_context::run()
[36m(RayTrainWorker pid=1349320)[0m [2025-02-07 04:15:53,437 E 1349320 1353719] logging.cc:460:     @     0x14ed42da3101       1280  ray::core::CoreWorker::RunIOService()
[36m(RayTrainWorker pid=1349320)[0m [2025-02-07 04:15:53,437 E 1349320 1353719] logging.cc:460:     @     0x14ed4325de90         64  thread_proxy
[36m(RayTrainWorker pid=1349320)[0m [2025-02-07 04:15:53,437 E 1349320 1353719] logging.cc:460:     @     0x14ed44ce21ca  (unknown)  start_thread
[36m(RayTrainWorker pid=1349320)[0m Fatal Python error: Segmentation fault
[36m(RayTrainWorker pid=1349320)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, ray._raylet, mkl._mklinit, mkl._py_mkl_service, numpy._core._multiarray_umath, numpy._core._multiarray_tests, numpy.linalg._umath_linalg, pyarrow.lib, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pyarrow._compute, pandas._libs.ops, pandas._libs.hashing, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.internals, pandas._libs.indexing, pandas._libs.index, pandas._libs.writers, pandas._libs.join, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, pyarrow._parquet, pyarrow._json, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, gmpy2.gmpy2, scipy._lib._ccallback_c, scipy.signal._sigtools, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.special._ufuncs_cxx, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.special._ellip_harm_2, scipy._lib._uarray._uarray, scipy.signal._max_len_seq_inner, scipy.signal._upfirdn_apply, scipy.signal._spline, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._distance_wrap, scipy.spatial._hausdorff, scipy.spatial.transform._rotation, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._cobyla, scipy.optimize._slsqp, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy.optimize._cython_nnls, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.interpolate._bspl, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, scipy.signal._sosfilt, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.special.cython_special, scipy.stats._stats, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.stats._mvn, scipy.stats._rcont.rcont, scipy.signal._peak_finding_utils, PIL._imaging, kiwisolver._cext, h5py._errors, h5py.defs, h5py._objects, h5py.h5, h5py.utils, h5py.h5t, h5py.h5s, h5py.h5ac, h5py.h5p, h5py.h5r, h5py._proxy, h5py._conv, h5py.h5z, h5py.h5a, h5py.h5d, h5py.h5ds, h5py.h5g, h5py.h5i, h5py.h5o, h5py.h5f, h5py.h5fd, h5py.h5pl, h5py.h5l, h5py._selector, sklearn.__check_build._check_build, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, sklearn.utils._random (total: 220)
[36m(RayTrainWorker pid=1712262)[0m Setting up process group for: env:// [rank=0, world_size=8]
[36m(RayTrainWorker pid=1349257)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00001_1_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000098)[32m [repeated 16x across cluster][0m
[36m(RayTrainWorker pid=1349314)[0m *** SIGSEGV received at time=1738919763 on cpu 17 ***
[36m(TorchTrainer pid=1709398)[0m Started distributed worker processes: 
[36m(TorchTrainer pid=1709398)[0m - (node_id=049cb75edba8a3001da8f4da1e35abcbde72dd6c7f1fae22bc2fcd05, ip=172.17.6.198, pid=1712262) world_rank=0, local_rank=0, node_rank=0
[36m(TorchTrainer pid=1709398)[0m - (node_id=049cb75edba8a3001da8f4da1e35abcbde72dd6c7f1fae22bc2fcd05, ip=172.17.6.198, pid=1712259) world_rank=1, local_rank=1, node_rank=0
[36m(TorchTrainer pid=1709398)[0m - (node_id=049cb75edba8a3001da8f4da1e35abcbde72dd6c7f1fae22bc2fcd05, ip=172.17.6.198, pid=1712264) world_rank=2, local_rank=2, node_rank=0
[36m(TorchTrainer pid=1709398)[0m - (node_id=049cb75edba8a3001da8f4da1e35abcbde72dd6c7f1fae22bc2fcd05, ip=172.17.6.198, pid=1712260) world_rank=3, local_rank=3, node_rank=0
[36m(TorchTrainer pid=1709398)[0m - (node_id=049cb75edba8a3001da8f4da1e35abcbde72dd6c7f1fae22bc2fcd05, ip=172.17.6.198, pid=1712261) world_rank=4, local_rank=4, node_rank=0
[36m(TorchTrainer pid=1709398)[0m - (node_id=049cb75edba8a3001da8f4da1e35abcbde72dd6c7f1fae22bc2fcd05, ip=172.17.6.198, pid=1712263) world_rank=5, local_rank=5, node_rank=0
[36m(TorchTrainer pid=1709398)[0m - (node_id=049cb75edba8a3001da8f4da1e35abcbde72dd6c7f1fae22bc2fcd05, ip=172.17.6.198, pid=1712265) world_rank=6, local_rank=6, node_rank=0
[36m(TorchTrainer pid=1709398)[0m - (node_id=049cb75edba8a3001da8f4da1e35abcbde72dd6c7f1fae22bc2fcd05, ip=172.17.6.198, pid=1712266) world_rank=7, local_rank=7, node_rank=0
[36m(RayTrainWorker pid=1349314)[0m PC: @     0x15087ff622fe  (unknown)  ray::gcs::TaskInfoAccessor::AsyncAddTaskEventData()
[36m(RayTrainWorker pid=1349314)[0m     @     0x150881c57d10       3648  (unknown)
[36m(RayTrainWorker pid=1349314)[0m     @     0x15087fe45045       1392  ray::core::worker::TaskEventBufferImpl::FlushEvents()
[36m(RayTrainWorker pid=1349314)[0m     @     0x15087fdd100c       1488  ray::core::CoreWorker::Disconnect()
[36m(RayTrainWorker pid=1349314)[0m     @     0x15087fdd13bd       1152  ray::core::CoreWorker::ForceExit()
[36m(RayTrainWorker pid=1349314)[0m     @     0x15087fdd17ef       1680  ray::core::CoreWorker::HandleKillActor()
[36m(RayTrainWorker pid=1349314)[0m     @     0x15087fdc8cf4        192  ray::rpc::ServerCallImpl<>::HandleRequestImpl()
[36m(RayTrainWorker pid=1349314)[0m     @     0x150880117728       1168  EventTracker::RecordExecution()
[36m(RayTrainWorker pid=1349314)[0m     @     0x1508801126fe         48  std::_Function_handler<>::_M_invoke()
[36m(RayTrainWorker pid=1349314)[0m     @     0x150880112b76        112  boost::asio::detail::completion_handler<>::do_complete()
[36m(RayTrainWorker pid=1349314)[0m     @     0x1508806e82bb        128  boost::asio::detail::scheduler::do_run_one()
[36m(RayTrainWorker pid=1349314)[0m     @     0x1508806e9c39        288  boost::asio::detail::scheduler::run()
[36m(RayTrainWorker pid=1349314)[0m     @     0x1508806ea342         96  boost::asio::io_context::run()
[36m(RayTrainWorker pid=1349314)[0m     @     0x15087fd0e101       1280  ray::core::CoreWorker::RunIOService()
[36m(RayTrainWorker pid=1349314)[0m     @     0x1508801c8e90         64  thread_proxy
[36m(RayTrainWorker pid=1349314)[0m     @     0x150881c4d1ca  (unknown)  start_thread
[36m(RayTrainWorker pid=1349314)[0m [2025-02-07 04:16:06,085 E 1349314 1350880] logging.cc:460: *** SIGSEGV received at time=1738919766 on cpu 17 ***
[36m(RayTrainWorker pid=1349314)[0m [2025-02-07 04:16:06,085 E 1349314 1350880] logging.cc:460: PC: @     0x15087ff622fe  (unknown)  ray::gcs::TaskInfoAccessor::AsyncAddTaskEventData()
[36m(RayTrainWorker pid=1349314)[0m [2025-02-07 04:16:06,085 E 1349314 1350880] logging.cc:460:     @     0x150881c57d10       3648  (unknown)
[36m(RayTrainWorker pid=1349314)[0m [2025-02-07 04:16:06,085 E 1349314 1350880] logging.cc:460:     @     0x15087fe45045       1392  ray::core::worker::TaskEventBufferImpl::FlushEvents()
[36m(RayTrainWorker pid=1349314)[0m [2025-02-07 04:16:06,085 E 1349314 1350880] logging.cc:460:     @     0x15087fdd100c       1488  ray::core::CoreWorker::Disconnect()
[36m(RayTrainWorker pid=1349314)[0m [2025-02-07 04:16:06,085 E 1349314 1350880] logging.cc:460:     @     0x15087fdd13bd       1152  ray::core::CoreWorker::ForceExit()
[36m(RayTrainWorker pid=1349314)[0m [2025-02-07 04:16:06,085 E 1349314 1350880] logging.cc:460:     @     0x15087fdd17ef       1680  ray::core::CoreWorker::HandleKillActor()
[36m(RayTrainWorker pid=1349314)[0m [2025-02-07 04:16:06,085 E 1349314 1350880] logging.cc:460:     @     0x15087fdc8cf4        192  ray::rpc::ServerCallImpl<>::HandleRequestImpl()
[36m(RayTrainWorker pid=1349314)[0m [2025-02-07 04:16:06,085 E 1349314 1350880] logging.cc:460:     @     0x150880117728       1168  EventTracker::RecordExecution()
[36m(RayTrainWorker pid=1349314)[0m [2025-02-07 04:16:06,085 E 1349314 1350880] logging.cc:460:     @     0x1508801126fe         48  std::_Function_handler<>::_M_invoke()
[36m(RayTrainWorker pid=1349314)[0m [2025-02-07 04:16:06,085 E 1349314 1350880] logging.cc:460:     @     0x150880112b76        112  boost::asio::detail::completion_handler<>::do_complete()
[36m(RayTrainWorker pid=1349314)[0m [2025-02-07 04:16:06,085 E 1349314 1350880] logging.cc:460:     @     0x1508806e82bb        128  boost::asio::detail::scheduler::do_run_one()
[36m(RayTrainWorker pid=1349314)[0m [2025-02-07 04:16:06,085 E 1349314 1350880] logging.cc:460:     @     0x1508806e9c39        288  boost::asio::detail::scheduler::run()
[36m(RayTrainWorker pid=1349314)[0m [2025-02-07 04:16:06,085 E 1349314 1350880] logging.cc:460:     @     0x1508806ea342         96  boost::asio::io_context::run()
[36m(RayTrainWorker pid=1349314)[0m [2025-02-07 04:16:06,085 E 1349314 1350880] logging.cc:460:     @     0x15087fd0e101       1280  ray::core::CoreWorker::RunIOService()
[36m(RayTrainWorker pid=1349314)[0m [2025-02-07 04:16:06,085 E 1349314 1350880] logging.cc:460:     @     0x1508801c8e90         64  thread_proxy
[36m(RayTrainWorker pid=1349314)[0m [2025-02-07 04:16:06,085 E 1349314 1350880] logging.cc:460:     @     0x150881c4d1ca  (unknown)  start_thread
[36m(RayTrainWorker pid=1349314)[0m Fatal Python error: Segmentation fault
[36m(RayTrainWorker pid=1349314)[0m 
[36m(RayTrainWorker pid=1349314)[0m 
[36m(RayTrainWorker pid=1349314)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, ray._raylet, mkl._mklinit, mkl._py_mkl_service, numpy._core._multiarray_umath, numpy._core._multiarray_tests, numpy.linalg._umath_linalg, pyarrow.lib, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pyarrow._compute, pandas._libs.ops, pandas._libs.hashing, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.internals, pandas._libs.indexing, pandas._libs.index, pandas._libs.writers, pandas._libs.join, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, pyarrow._parquet, pyarrow._json, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, gmpy2.gmpy2, scipy._lib._ccallback_c, scipy.signal._sigtools, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.special._ufuncs_cxx, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.special._ellip_harm_2, scipy._lib._uarray._uarray, scipy.signal._max_len_seq_inner, scipy.signal._upfirdn_apply, scipy.signal._spline, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._distance_wrap, scipy.spatial._hausdorff, scipy.spatial.transform._rotation, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._cobyla, scipy.optimize._slsqp, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy.optimize._cython_nnls, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.interpolate._bspl, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, scipy.signal._sosfilt, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.special.cython_special, scipy.stats._stats, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.stats._mvn, scipy.stats._rcont.rcont, scipy.signal._peak_finding_utils, PIL._imaging, kiwisolver._cext, h5py._errors, h5py.defs, h5py._objects, h5py.h5, h5py.utils, h5py.h5t, h5py.h5s, h5py.h5ac, h5py.h5p, h5py.h5r, h5py._proxy, h5py._conv, h5py.h5z, h5py.h5a, h5py.h5d, h5py.h5ds, h5py.h5g, h5py.h5i, h5py.h5o, h5py.h5f, h5py.h5fd, h5py.h5pl, h5py.h5l, h5py._selector, sklearn.__check_build._check_build, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, sklearn.utils._random (total: 220)
[36m(RayTrainWorker pid=1712262)[0m GPU available: False, used: False
[36m(RayTrainWorker pid=1712262)[0m TPU available: False, using: 0 TPU cores
[36m(RayTrainWorker pid=1712262)[0m HPU available: False, using: 0 HPUs
[36m(RayTrainWorker pid=1712262)[0m /home/ad2002/.conda/envs/neuralnets/lib/python3.12/site-packages/lightning/pytorch/loops/utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[36m(RayTrainWorker pid=1712262)[0m 
[36m(RayTrainWorker pid=1712262)[0m   | Name  | Type  | Params | Mode 
[36m(RayTrainWorker pid=1712262)[0m ----------------------------------------
[36m(RayTrainWorker pid=1712262)[0m 0 | model | frRNN | 175    | train
[36m(RayTrainWorker pid=1712262)[0m ----------------------------------------
[36m(RayTrainWorker pid=1712262)[0m 105       Trainable params
[36m(RayTrainWorker pid=1712262)[0m 70        Non-trainable params
[36m(RayTrainWorker pid=1712262)[0m 175       Total params
[36m(RayTrainWorker pid=1712262)[0m 0.001     Total estimated model params size (MB)
[36m(RayTrainWorker pid=1712262)[0m 1         Modules in train mode
[36m(RayTrainWorker pid=1712262)[0m 0         Modules in eval mode
[36m(RayTrainWorker pid=1349246)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00003_3_lr=0.0001,noise_std=0.1000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000099)[32m [repeated 17x across cluster][0m
[36m(RayTrainWorker pid=1712262)[0m /home/ad2002/.conda/envs/neuralnets/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[36m(TorchTrainer pid=1716345)[0m GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.
[36m(RayTrainWorker pid=1712266)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00005_5_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000000)[32m [repeated 31x across cluster][0m
[36m(RayTrainWorker pid=1720535)[0m Setting up process group for: env:// [rank=0, world_size=8]
[36m(TorchTrainer pid=1718142)[0m GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.
[36m(RayTrainWorker pid=1349250)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00004_4_lr=0.0010,noise_std=0.0000,weight_decay=0.0000_2025-02-07_04-03-23/checkpoint_000099)[32m [repeated 8x across cluster][0m
[36m(TorchTrainer pid=1716345)[0m Started distributed worker processes: 
[36m(TorchTrainer pid=1716345)[0m - (node_id=049cb75edba8a3001da8f4da1e35abcbde72dd6c7f1fae22bc2fcd05, ip=172.17.6.198, pid=1720535) world_rank=0, local_rank=0, node_rank=0
[36m(TorchTrainer pid=1716345)[0m - (node_id=049cb75edba8a3001da8f4da1e35abcbde72dd6c7f1fae22bc2fcd05, ip=172.17.6.198, pid=1720533) world_rank=1, local_rank=1, node_rank=0
[36m(TorchTrainer pid=1716345)[0m - (node_id=049cb75edba8a3001da8f4da1e35abcbde72dd6c7f1fae22bc2fcd05, ip=172.17.6.198, pid=1720542) world_rank=2, local_rank=2, node_rank=0
[36m(TorchTrainer pid=1716345)[0m - (node_id=049cb75edba8a3001da8f4da1e35abcbde72dd6c7f1fae22bc2fcd05, ip=172.17.6.198, pid=1720541) world_rank=3, local_rank=3, node_rank=0
[36m(TorchTrainer pid=1716345)[0m - (node_id=049cb75edba8a3001da8f4da1e35abcbde72dd6c7f1fae22bc2fcd05, ip=172.17.6.198, pid=1720537) world_rank=4, local_rank=4, node_rank=0
[36m(TorchTrainer pid=1716345)[0m - (node_id=049cb75edba8a3001da8f4da1e35abcbde72dd6c7f1fae22bc2fcd05, ip=172.17.6.198, pid=1720539) world_rank=5, local_rank=5, node_rank=0
[36m(TorchTrainer pid=1716345)[0m - (node_id=049cb75edba8a3001da8f4da1e35abcbde72dd6c7f1fae22bc2fcd05, ip=172.17.6.198, pid=1720538) world_rank=6, local_rank=6, node_rank=0
[36m(TorchTrainer pid=1716345)[0m - (node_id=049cb75edba8a3001da8f4da1e35abcbde72dd6c7f1fae22bc2fcd05, ip=172.17.6.198, pid=1720540) world_rank=7, local_rank=7, node_rank=0
[36m(RayTrainWorker pid=1720535)[0m GPU available: False, used: False
[36m(RayTrainWorker pid=1720535)[0m TPU available: False, using: 0 TPU cores
[36m(RayTrainWorker pid=1720535)[0m HPU available: False, using: 0 HPUs
[36m(RayTrainWorker pid=1720535)[0m /home/ad2002/.conda/envs/neuralnets/lib/python3.12/site-packages/lightning/pytorch/loops/utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[36m(RayTrainWorker pid=1720535)[0m 
[36m(RayTrainWorker pid=1720535)[0m   | Name  | Type  | Params | Mode 
[36m(RayTrainWorker pid=1720535)[0m ----------------------------------------
[36m(RayTrainWorker pid=1720535)[0m 0 | model | frRNN | 175    | train
[36m(RayTrainWorker pid=1720535)[0m ----------------------------------------
[36m(RayTrainWorker pid=1720535)[0m 105       Trainable params
[36m(RayTrainWorker pid=1720535)[0m 70        Non-trainable params
[36m(RayTrainWorker pid=1720535)[0m 175       Total params
[36m(RayTrainWorker pid=1720535)[0m 0.001     Total estimated model params size (MB)
[36m(RayTrainWorker pid=1720535)[0m 1         Modules in train mode
[36m(RayTrainWorker pid=1720535)[0m 0         Modules in eval mode
[36m(RayTrainWorker pid=1721199)[0m Setting up process group for: env:// [rank=0, world_size=8]
[36m(RayTrainWorker pid=1712266)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00005_5_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000001)[32m [repeated 8x across cluster][0m
[36m(RayTrainWorker pid=1720535)[0m /home/ad2002/.conda/envs/neuralnets/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[36m(TorchTrainer pid=1718142)[0m Started distributed worker processes: 
[36m(TorchTrainer pid=1718142)[0m - (node_id=049cb75edba8a3001da8f4da1e35abcbde72dd6c7f1fae22bc2fcd05, ip=172.17.6.198, pid=1721198) world_rank=7, local_rank=7, node_rank=0[32m [repeated 8x across cluster][0m
[36m(RayTrainWorker pid=1721199)[0m 
[36m(RayTrainWorker pid=1721199)[0m GPU available: False, used: False
[36m(RayTrainWorker pid=1721199)[0m TPU available: False, using: 0 TPU cores
[36m(RayTrainWorker pid=1721199)[0m HPU available: False, using: 0 HPUs
[36m(RayTrainWorker pid=1721199)[0m /home/ad2002/.conda/envs/neuralnets/lib/python3.12/site-packages/lightning/pytorch/loops/utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[36m(RayTrainWorker pid=1721199)[0m   | Name  | Type  | Params | Mode 
[36m(RayTrainWorker pid=1721199)[0m ----------------------------------------[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1721199)[0m 0 | model | frRNN | 175    | train
[36m(RayTrainWorker pid=1721199)[0m 105       Trainable params
[36m(RayTrainWorker pid=1721199)[0m 70        Non-trainable params
[36m(RayTrainWorker pid=1721199)[0m 175       Total params
[36m(RayTrainWorker pid=1721199)[0m 0.001     Total estimated model params size (MB)
[36m(RayTrainWorker pid=1721199)[0m 1         Modules in train mode
[36m(RayTrainWorker pid=1721199)[0m 0         Modules in eval mode
[36m(RayTrainWorker pid=1721199)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00007_7_lr=0.0010,noise_std=0.1000,weight_decay=0.0000_2025-02-07_04-03-23/checkpoint_000000)[32m [repeated 25x across cluster][0m
[36m(RayTrainWorker pid=1721199)[0m /home/ad2002/.conda/envs/neuralnets/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[36m(RayTrainWorker pid=1712262)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00005_5_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000005)[32m [repeated 32x across cluster][0m
[36m(RayTrainWorker pid=1720535)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00006_6_lr=0.0001,noise_std=0.0000,weight_decay=0.0000_2025-02-07_04-03-23/checkpoint_000004)[32m [repeated 48x across cluster][0m
2025-02-07 04:16:46,702	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(RayTrainWorker pid=1712262)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00005_5_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000008)[32m [repeated 32x across cluster][0m
2025-02-07 04:16:51,305	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(RayTrainWorker pid=1720535)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00006_6_lr=0.0001,noise_std=0.0000,weight_decay=0.0000_2025-02-07_04-03-23/checkpoint_000007)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1721199)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00007_7_lr=0.0010,noise_std=0.1000,weight_decay=0.0000_2025-02-07_04-03-23/checkpoint_000008)[32m [repeated 32x across cluster][0m
2025-02-07 04:17:07,485	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(RayTrainWorker pid=1712262)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00005_5_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000013)[32m [repeated 48x across cluster][0m
[36m(RayTrainWorker pid=1720535)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00006_6_lr=0.0001,noise_std=0.0000,weight_decay=0.0000_2025-02-07_04-03-23/checkpoint_000012)[32m [repeated 32x across cluster][0m
2025-02-07 04:17:22,271	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(RayTrainWorker pid=1720535)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00006_6_lr=0.0001,noise_std=0.0000,weight_decay=0.0000_2025-02-07_04-03-23/checkpoint_000014)[32m [repeated 48x across cluster][0m
2025-02-07 04:17:26,762	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(RayTrainWorker pid=1712262)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00005_5_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000018)[32m [repeated 32x across cluster][0m
[36m(RayTrainWorker pid=1712262)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00005_5_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000020)[32m [repeated 48x across cluster][0m
[36m(RayTrainWorker pid=1720542)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00006_6_lr=0.0001,noise_std=0.0000,weight_decay=0.0000_2025-02-07_04-03-23/checkpoint_000019)[32m [repeated 32x across cluster][0m
[36m(RayTrainWorker pid=1720535)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00006_6_lr=0.0001,noise_std=0.0000,weight_decay=0.0000_2025-02-07_04-03-23/checkpoint_000020)[32m [repeated 24x across cluster][0m
2025-02-07 04:17:54,212	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(RayTrainWorker pid=1720535)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00006_6_lr=0.0001,noise_std=0.0000,weight_decay=0.0000_2025-02-07_04-03-23/checkpoint_000022)[32m [repeated 48x across cluster][0m
2025-02-07 04:17:58,590	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(RayTrainWorker pid=1721199)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00007_7_lr=0.0010,noise_std=0.1000,weight_decay=0.0000_2025-02-07_04-03-23/checkpoint_000021)[32m [repeated 32x across cluster][0m
[36m(RayTrainWorker pid=1721199)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00007_7_lr=0.0010,noise_std=0.1000,weight_decay=0.0000_2025-02-07_04-03-23/checkpoint_000023)[32m [repeated 48x across cluster][0m
[36m(RayTrainWorker pid=1712262)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00005_5_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000029)[32m [repeated 32x across cluster][0m
2025-02-07 04:18:22,069	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(RayTrainWorker pid=1721199)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00007_7_lr=0.0010,noise_std=0.1000,weight_decay=0.0000_2025-02-07_04-03-23/checkpoint_000026)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1712262)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00005_5_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000032)[32m [repeated 32x across cluster][0m
[36m(RayTrainWorker pid=1720535)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00006_6_lr=0.0001,noise_std=0.0000,weight_decay=0.0000_2025-02-07_04-03-23/checkpoint_000031)[32m [repeated 32x across cluster][0m
[36m(RayTrainWorker pid=1712262)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00005_5_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000035)[32m [repeated 40x across cluster][0m
2025-02-07 04:18:44,052	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2025-02-07 04:18:48,633	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(RayTrainWorker pid=1712262)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00005_5_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000037)[32m [repeated 32x across cluster][0m
2025-02-07 04:18:52,962	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(RayTrainWorker pid=1712262)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00005_5_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000039)[32m [repeated 48x across cluster][0m
2025-02-07 04:18:57,227	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(RayTrainWorker pid=1720535)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00006_6_lr=0.0001,noise_std=0.0000,weight_decay=0.0000_2025-02-07_04-03-23/checkpoint_000037)[32m [repeated 32x across cluster][0m
[36m(RayTrainWorker pid=1721199)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00007_7_lr=0.0010,noise_std=0.1000,weight_decay=0.0000_2025-02-07_04-03-23/checkpoint_000037)[32m [repeated 48x across cluster][0m
[36m(RayTrainWorker pid=1720535)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00006_6_lr=0.0001,noise_std=0.0000,weight_decay=0.0000_2025-02-07_04-03-23/checkpoint_000040)[32m [repeated 32x across cluster][0m
2025-02-07 04:19:25,218	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(RayTrainWorker pid=1712262)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00005_5_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000046)[32m [repeated 48x across cluster][0m
2025-02-07 04:19:29,166	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(RayTrainWorker pid=1720535)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00006_6_lr=0.0001,noise_std=0.0000,weight_decay=0.0000_2025-02-07_04-03-23/checkpoint_000044)[32m [repeated 48x across cluster][0m
2025-02-07 04:19:33,676	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(RayTrainWorker pid=1712262)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00005_5_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000049)[32m [repeated 32x across cluster][0m
2025-02-07 04:19:38,554	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(RayTrainWorker pid=1721199)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00007_7_lr=0.0010,noise_std=0.1000,weight_decay=0.0000_2025-02-07_04-03-23/checkpoint_000045)[32m [repeated 32x across cluster][0m
[36m(RayTrainWorker pid=1720535)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00006_6_lr=0.0001,noise_std=0.0000,weight_decay=0.0000_2025-02-07_04-03-23/checkpoint_000048)[32m [repeated 32x across cluster][0m
2025-02-07 04:19:49,412	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(RayTrainWorker pid=1721199)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00007_7_lr=0.0010,noise_std=0.1000,weight_decay=0.0000_2025-02-07_04-03-23/checkpoint_000048)[32m [repeated 40x across cluster][0m
2025-02-07 04:19:54,410	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(RayTrainWorker pid=1720535)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00006_6_lr=0.0001,noise_std=0.0000,weight_decay=0.0000_2025-02-07_04-03-23/checkpoint_000051)[32m [repeated 32x across cluster][0m
[36m(RayTrainWorker pid=1721199)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00007_7_lr=0.0010,noise_std=0.1000,weight_decay=0.0000_2025-02-07_04-03-23/checkpoint_000051)[32m [repeated 32x across cluster][0m
2025-02-07 04:20:11,086	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(RayTrainWorker pid=1720535)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00006_6_lr=0.0001,noise_std=0.0000,weight_decay=0.0000_2025-02-07_04-03-23/checkpoint_000054)[32m [repeated 40x across cluster][0m
2025-02-07 04:20:15,472	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(RayTrainWorker pid=1721199)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00007_7_lr=0.0010,noise_std=0.1000,weight_decay=0.0000_2025-02-07_04-03-23/checkpoint_000054)[32m [repeated 32x across cluster][0m
[36m(RayTrainWorker pid=1712262)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00005_5_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000060)[32m [repeated 32x across cluster][0m
2025-02-07 04:20:25,236	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(RayTrainWorker pid=1721191)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00007_7_lr=0.0010,noise_std=0.1000,weight_decay=0.0000_2025-02-07_04-03-23/checkpoint_000057)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1712262)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00005_5_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000063)[32m [repeated 32x across cluster][0m
[36m(RayTrainWorker pid=1720535)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00006_6_lr=0.0001,noise_std=0.0000,weight_decay=0.0000_2025-02-07_04-03-23/checkpoint_000061)[32m [repeated 32x across cluster][0m
2025-02-07 04:20:47,009	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(RayTrainWorker pid=1712262)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00005_5_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000066)[32m [repeated 40x across cluster][0m
2025-02-07 04:20:51,767	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(RayTrainWorker pid=1720535)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00006_6_lr=0.0001,noise_std=0.0000,weight_decay=0.0000_2025-02-07_04-03-23/checkpoint_000064)[32m [repeated 32x across cluster][0m
[36m(RayTrainWorker pid=1712262)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00005_5_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000069)[32m [repeated 32x across cluster][0m
[36m(RayTrainWorker pid=1712262)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00005_5_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000071)[32m [repeated 48x across cluster][0m
2025-02-07 04:21:09,966	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2025-02-07 04:21:14,181	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(RayTrainWorker pid=1720535)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00006_6_lr=0.0001,noise_std=0.0000,weight_decay=0.0000_2025-02-07_04-03-23/checkpoint_000070)[32m [repeated 48x across cluster][0m
[36m(RayTrainWorker pid=1720535)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00006_6_lr=0.0001,noise_std=0.0000,weight_decay=0.0000_2025-02-07_04-03-23/checkpoint_000072)[32m [repeated 48x across cluster][0m
2025-02-07 04:21:26,832	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2025-02-07 04:21:31,271	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(RayTrainWorker pid=1720535)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00006_6_lr=0.0001,noise_std=0.0000,weight_decay=0.0000_2025-02-07_04-03-23/checkpoint_000074)[32m [repeated 48x across cluster][0m
[36m(RayTrainWorker pid=1712262)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00005_5_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000078)[32m [repeated 40x across cluster][0m
2025-02-07 04:21:43,483	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(RayTrainWorker pid=1720535)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00006_6_lr=0.0001,noise_std=0.0000,weight_decay=0.0000_2025-02-07_04-03-23/checkpoint_000077)[32m [repeated 32x across cluster][0m
2025-02-07 04:21:48,001	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2025-02-07 04:21:52,630	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(RayTrainWorker pid=1720535)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00006_6_lr=0.0001,noise_std=0.0000,weight_decay=0.0000_2025-02-07_04-03-23/checkpoint_000079)[32m [repeated 48x across cluster][0m
[36m(RayTrainWorker pid=1712262)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00005_5_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000083)[32m [repeated 32x across cluster][0m
2025-02-07 04:22:10,426	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(RayTrainWorker pid=1712262)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00005_5_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000085)[32m [repeated 48x across cluster][0m
2025-02-07 04:22:14,580	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2025-02-07 04:22:19,216	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(RayTrainWorker pid=1712262)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00005_5_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000087)[32m [repeated 48x across cluster][0m
2025-02-07 04:22:23,381	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(RayTrainWorker pid=1720535)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00006_6_lr=0.0001,noise_std=0.0000,weight_decay=0.0000_2025-02-07_04-03-23/checkpoint_000087)[32m [repeated 48x across cluster][0m
2025-02-07 04:22:27,707	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(RayTrainWorker pid=1721199)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00007_7_lr=0.0010,noise_std=0.1000,weight_decay=0.0000_2025-02-07_04-03-23/checkpoint_000085)[32m [repeated 32x across cluster][0m
2025-02-07 04:22:37,437	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(RayTrainWorker pid=1721191)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00007_7_lr=0.0010,noise_std=0.1000,weight_decay=0.0000_2025-02-07_04-03-23/checkpoint_000087)[32m [repeated 48x across cluster][0m
2025-02-07 04:22:42,156	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(RayTrainWorker pid=1720535)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00006_6_lr=0.0001,noise_std=0.0000,weight_decay=0.0000_2025-02-07_04-03-23/checkpoint_000092)[32m [repeated 32x across cluster][0m
[36m(RayTrainWorker pid=1720535)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00006_6_lr=0.0001,noise_std=0.0000,weight_decay=0.0000_2025-02-07_04-03-23/checkpoint_000094)[32m [repeated 48x across cluster][0m
[36m(RayTrainWorker pid=1712262)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00005_5_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000096)[32m [repeated 32x across cluster][0m
2025-02-07 04:23:05,340	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(RayTrainWorker pid=1720535)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00006_6_lr=0.0001,noise_std=0.0000,weight_decay=0.0000_2025-02-07_04-03-23/checkpoint_000098)[32m [repeated 48x across cluster][0m
2025-02-07 04:23:09,429	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2025-02-07 04:23:13,944	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(TorchTrainer pid=1709398)[0m *** SIGSEGV received at time=1738920194 on cpu 1 ***
[36m(TorchTrainer pid=1709398)[0m PC: @     0x1520170cb2fe  (unknown)  ray::gcs::TaskInfoAccessor::AsyncAddTaskEventData()
[36m(TorchTrainer pid=1709398)[0m     @     0x152018dc0d10       3648  (unknown)
[36m(TorchTrainer pid=1709398)[0m     @     0x152016fae045       1392  ray::core::worker::TaskEventBufferImpl::FlushEvents()
[36m(TorchTrainer pid=1709398)[0m     @     0x152016f3a00c       1488  ray::core::CoreWorker::Disconnect()
[36m(TorchTrainer pid=1709398)[0m     @     0x152016f3a3bd       1152  ray::core::CoreWorker::ForceExit()
[36m(TorchTrainer pid=1709398)[0m     @     0x152016f3a7ef       1680  ray::core::CoreWorker::HandleKillActor()
[36m(TorchTrainer pid=1709398)[0m     @     0x152016f31cf4        192  ray::rpc::ServerCallImpl<>::HandleRequestImpl()
[36m(TorchTrainer pid=1709398)[0m     @     0x152017280728       1168  EventTracker::RecordExecution()
[36m(TorchTrainer pid=1709398)[0m     @     0x15201727b6fe         48  std::_Function_handler<>::_M_invoke()
[36m(TorchTrainer pid=1709398)[0m     @     0x15201727bb76        112  boost::asio::detail::completion_handler<>::do_complete()
[36m(TorchTrainer pid=1709398)[0m     @     0x1520178512bb        128  boost::asio::detail::scheduler::do_run_one()
[36m(TorchTrainer pid=1709398)[0m     @     0x152017852c39        288  boost::asio::detail::scheduler::run()
[36m(TorchTrainer pid=1709398)[0m     @     0x152017853342         96  boost::asio::io_context::run()
[36m(TorchTrainer pid=1709398)[0m     @     0x152016e77101       1280  ray::core::CoreWorker::RunIOService()
[36m(TorchTrainer pid=1709398)[0m     @     0x152017331e90         64  thread_proxy
[36m(TorchTrainer pid=1709398)[0m     @     0x152018db61ca  (unknown)  start_thread
[36m(TorchTrainer pid=1709398)[0m [2025-02-07 04:23:14,212 E 1709398 1709857] logging.cc:460: *** SIGSEGV received at time=1738920194 on cpu 1 ***
[36m(TorchTrainer pid=1709398)[0m [2025-02-07 04:23:14,212 E 1709398 1709857] logging.cc:460: PC: @     0x1520170cb2fe  (unknown)  ray::gcs::TaskInfoAccessor::AsyncAddTaskEventData()
[36m(TorchTrainer pid=1709398)[0m [2025-02-07 04:23:14,212 E 1709398 1709857] logging.cc:460:     @     0x152018dc0d10       3648  (unknown)
[36m(TorchTrainer pid=1709398)[0m [2025-02-07 04:23:14,213 E 1709398 1709857] logging.cc:460:     @     0x152016fae045       1392  ray::core::worker::TaskEventBufferImpl::FlushEvents()
[36m(TorchTrainer pid=1709398)[0m [2025-02-07 04:23:14,213 E 1709398 1709857] logging.cc:460:     @     0x152016f3a00c       1488  ray::core::CoreWorker::Disconnect()
[36m(TorchTrainer pid=1709398)[0m [2025-02-07 04:23:14,213 E 1709398 1709857] logging.cc:460:     @     0x152016f3a3bd       1152  ray::core::CoreWorker::ForceExit()
[36m(TorchTrainer pid=1709398)[0m [2025-02-07 04:23:14,213 E 1709398 1709857] logging.cc:460:     @     0x152016f3a7ef       1680  ray::core::CoreWorker::HandleKillActor()
[36m(TorchTrainer pid=1709398)[0m [2025-02-07 04:23:14,213 E 1709398 1709857] logging.cc:460:     @     0x152016f31cf4        192  ray::rpc::ServerCallImpl<>::HandleRequestImpl()
[36m(TorchTrainer pid=1709398)[0m [2025-02-07 04:23:14,213 E 1709398 1709857] logging.cc:460:     @     0x152017280728       1168  EventTracker::RecordExecution()
[36m(TorchTrainer pid=1709398)[0m [2025-02-07 04:23:14,213 E 1709398 1709857] logging.cc:460:     @     0x15201727b6fe         48  std::_Function_handler<>::_M_invoke()
[36m(TorchTrainer pid=1709398)[0m [2025-02-07 04:23:14,213 E 1709398 1709857] logging.cc:460:     @     0x15201727bb76        112  boost::asio::detail::completion_handler<>::do_complete()
[36m(TorchTrainer pid=1709398)[0m [2025-02-07 04:23:14,213 E 1709398 1709857] logging.cc:460:     @     0x1520178512bb        128  boost::asio::detail::scheduler::do_run_one()
[36m(TorchTrainer pid=1709398)[0m [2025-02-07 04:23:14,213 E 1709398 1709857] logging.cc:460:     @     0x152017852c39        288  boost::asio::detail::scheduler::run()
[36m(TorchTrainer pid=1709398)[0m [2025-02-07 04:23:14,213 E 1709398 1709857] logging.cc:460:     @     0x152017853342         96  boost::asio::io_context::run()
[36m(TorchTrainer pid=1709398)[0m [2025-02-07 04:23:14,213 E 1709398 1709857] logging.cc:460:     @     0x152016e77101       1280  ray::core::CoreWorker::RunIOService()
[36m(TorchTrainer pid=1709398)[0m [2025-02-07 04:23:14,213 E 1709398 1709857] logging.cc:460:     @     0x152017331e90         64  thread_proxy
[36m(TorchTrainer pid=1709398)[0m [2025-02-07 04:23:14,213 E 1709398 1709857] logging.cc:460:     @     0x152018db61ca  (unknown)  start_thread
[36m(TorchTrainer pid=1709398)[0m Fatal Python error: Segmentation fault
[36m(TorchTrainer pid=1709398)[0m 
[36m(TorchTrainer pid=1709398)[0m 
[36m(TorchTrainer pid=1709398)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, ray._raylet, mkl._mklinit, mkl._py_mkl_service, numpy._core._multiarray_umath, numpy._core._multiarray_tests, numpy.linalg._umath_linalg, pyarrow.lib, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pyarrow._compute, pandas._libs.ops, pandas._libs.hashing, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.internals, pandas._libs.indexing, pandas._libs.index, pandas._libs.writers, pandas._libs.join, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, pyarrow._parquet, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, gmpy2.gmpy2, pyarrow._json, scipy._lib._ccallback_c, scipy.signal._sigtools, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.special._ufuncs_cxx, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.special._ellip_harm_2, scipy._lib._uarray._uarray, scipy.signal._max_len_seq_inner, scipy.signal._upfirdn_apply, scipy.signal._spline, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._distance_wrap, scipy.spatial._hausdorff, scipy.spatial.transform._rotation, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._cobyla, scipy.optimize._slsqp, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy.optimize._cython_nnls, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.interpolate._bspl, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, scipy.signal._sosfilt, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.special.cython_special, scipy.stats._stats, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.stats._mvn, scipy.stats._rcont.rcont, scipy.signal._peak_finding_utils, PIL._imaging, kiwisolver._cext, h5py._errors, h5py.defs, h5py._objects, h5py.h5, h5py.utils, h5py.h5t, h5py.h5s, h5py.h5ac, h5py.h5p, h5py.h5r, h5py._proxy, h5py._conv, h5py.h5z, h5py.h5a, h5py.h5d, h5py.h5ds, h5py.h5g, h5py.h5i, h5py.h5o, h5py.h5f, h5py.h5fd, h5py.h5pl, h5py.h5l, h5py._selector, sklearn.__check_build._check_build, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, sklearn.utils._random (total: 220)
[36m(RayTrainWorker pid=1712266)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00005_5_lr=0.0001,noise_std=0.0000,weight_decay=0.0010_2025-02-07_04-03-23/checkpoint_000099)[32m [repeated 47x across cluster][0m
[36m(RayTrainWorker pid=1712261)[0m 
[36m(RayTrainWorker pid=1712261)[0m 
[36m(RayTrainWorker pid=1712261)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, ray._raylet, mkl._mklinit, mkl._py_mkl_service, numpy._core._multiarray_umath, numpy._core._multiarray_tests, numpy.linalg._umath_linalg, pyarrow.lib, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pyarrow._compute, pandas._libs.ops, pandas._libs.hashing, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.internals, pandas._libs.indexing, pandas._libs.index, pandas._libs.writers, pandas._libs.join, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, pyarrow._parquet, pyarrow._json, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, gmpy2.gmpy2, scipy._lib._ccallback_c, scipy.signal._sigtools, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.special._ufuncs_cxx, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.special._ellip_harm_2, scipy._lib._uarray._uarray, scipy.signal._max_len_seq_inner, scipy.signal._upfirdn_apply, scipy.signal._spline, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._distance_wrap, scipy.spatial._hausdorff, scipy.spatial.transform._rotation, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._cobyla, scipy.optimize._slsqp, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy.optimize._cython_nnls, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.interpolate._bspl, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, scipy.signal._sosfilt, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.special.cython_special, scipy.stats._stats, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.stats._mvn, scipy.stats._rcont.rcont, scipy.signal._peak_finding_utils, PIL._imaging, kiwisolver._cext, h5py._errors, h5py.defs, h5py._objects, h5py.h5, h5py.utils, h5py.h5t, h5py.h5s, h5py.h5ac, h5py.h5p, h5py.h5r, h5py._proxy, h5py._conv, h5py.h5z, h5py.h5a, h5py.h5d, h5py.h5ds, h5py.h5g, h5py.h5i, h5py.h5o, h5py.h5f, h5py.h5fd, h5py.h5pl, h5py.h5l, h5py._selector, sklearn.__check_build._check_build, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, sklearn.utils._random (total: 220)
[36m(RayTrainWorker pid=1720535)[0m 
[36m(RayTrainWorker pid=1720535)[0m 
2025-02-07 04:23:17,979	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
[36m(RayTrainWorker pid=1720535)[0m *** SIGSEGV received at time=1738920195 on cpu 15 ***[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1720535)[0m PC: @     0x14f4ce0e12fe  (unknown)  ray::gcs::TaskInfoAccessor::AsyncAddTaskEventData()[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1720535)[0m     @     0x14f4cfdd6d10       3648  (unknown)[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1720535)[0m     @     0x14f4cdfc4045       1392  ray::core::worker::TaskEventBufferImpl::FlushEvents()[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1720535)[0m     @     0x14f4cdf5000c       1488  ray::core::CoreWorker::Disconnect()[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1712261)[0m     @     0x1507b00ad3bd       1152  ray::core::CoreWorker::ForceExit()[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1712261)[0m     @     0x1507b00ad7ef       1680  ray::core::CoreWorker::HandleKillActor()[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1712261)[0m     @     0x1507b00a4cf4        192  ray::rpc::ServerCallImpl<>::HandleRequestImpl()[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1712261)[0m     @     0x1507b03f3728       1168  EventTracker::RecordExecution()[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1712261)[0m     @     0x1507b03ee6fe         48  std::_Function_handler<>::_M_invoke()[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1712261)[0m     @     0x1507b03eeb76        112  boost::asio::detail::completion_handler<>::do_complete()[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1720535)[0m     @     0x14f4ce8672bb        128  boost::asio::detail::scheduler::do_run_one()[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1720535)[0m     @     0x14f4ce868c39        288  boost::asio::detail::scheduler::run()[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1720535)[0m     @     0x14f4ce869342         96  boost::asio::io_context::run()[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1720535)[0m     @     0x14f4cde8d101       1280  ray::core::CoreWorker::RunIOService()[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1720535)[0m     @     0x14f4ce347e90         64  thread_proxy[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1720535)[0m     @     0x14f4cfdcc1ca  (unknown)  start_thread[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1720535)[0m [2025-02-07 04:23:15,459 E 1720535 1720637] logging.cc:460: *** SIGSEGV received at time=1738920195 on cpu 15 ***[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1720535)[0m [2025-02-07 04:23:15,459 E 1720535 1720637] logging.cc:460: PC: @     0x14f4ce0e12fe  (unknown)  ray::gcs::TaskInfoAccessor::AsyncAddTaskEventData()[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1720535)[0m [2025-02-07 04:23:15,459 E 1720535 1720637] logging.cc:460:     @     0x14f4cfdd6d10       3648  (unknown)[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1720535)[0m [2025-02-07 04:23:15,459 E 1720535 1720637] logging.cc:460:     @     0x14f4cdfc4045       1392  ray::core::worker::TaskEventBufferImpl::FlushEvents()[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1720535)[0m [2025-02-07 04:23:15,459 E 1720535 1720637] logging.cc:460:     @     0x14f4cdf5000c       1488  ray::core::CoreWorker::Disconnect()[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1720535)[0m [2025-02-07 04:23:15,459 E 1720535 1720637] logging.cc:460:     @     0x14f4cdf503bd       1152  ray::core::CoreWorker::ForceExit()[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1720535)[0m [2025-02-07 04:23:15,459 E 1720535 1720637] logging.cc:460:     @     0x14f4cdf507ef       1680  ray::core::CoreWorker::HandleKillActor()[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1720535)[0m [2025-02-07 04:23:15,459 E 1720535 1720637] logging.cc:460:     @     0x14f4cdf47cf4        192  ray::rpc::ServerCallImpl<>::HandleRequestImpl()[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1720535)[0m [2025-02-07 04:23:15,459 E 1720535 1720637] logging.cc:460:     @     0x14f4ce296728       1168  EventTracker::RecordExecution()[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1720535)[0m [2025-02-07 04:23:15,459 E 1720535 1720637] logging.cc:460:     @     0x14f4ce2916fe         48  std::_Function_handler<>::_M_invoke()[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1720535)[0m [2025-02-07 04:23:15,459 E 1720535 1720637] logging.cc:460:     @     0x14f4ce291b76        112  boost::asio::detail::completion_handler<>::do_complete()[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1720535)[0m [2025-02-07 04:23:15,459 E 1720535 1720637] logging.cc:460:     @     0x14f4ce8672bb        128  boost::asio::detail::scheduler::do_run_one()[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1720535)[0m [2025-02-07 04:23:15,459 E 1720535 1720637] logging.cc:460:     @     0x14f4ce868c39        288  boost::asio::detail::scheduler::run()[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1720535)[0m [2025-02-07 04:23:15,459 E 1720535 1720637] logging.cc:460:     @     0x14f4ce869342         96  boost::asio::io_context::run()[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1720535)[0m [2025-02-07 04:23:15,459 E 1720535 1720637] logging.cc:460:     @     0x14f4cde8d101       1280  ray::core::CoreWorker::RunIOService()[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1720535)[0m [2025-02-07 04:23:15,459 E 1720535 1720637] logging.cc:460:     @     0x14f4ce347e90         64  thread_proxy[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1720535)[0m [2025-02-07 04:23:15,459 E 1720535 1720637] logging.cc:460:     @     0x14f4cfdcc1ca  (unknown)  start_thread[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1720535)[0m Fatal Python error: Segmentation fault[32m [repeated 2x across cluster][0m
2025-02-07 04:23:20,716	WARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.
You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.
You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.
2025-02-07 04:23:20,721	INFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19' in 0.0049s.
[36m(RayTrainWorker pid=1721199)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00007_7_lr=0.0010,noise_std=0.1000,weight_decay=0.0000_2025-02-07_04-03-23/checkpoint_000099)[32m [repeated 25x across cluster][0m
[36m(RayTrainWorker pid=1720535)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, ray._raylet, mkl._mklinit, mkl._py_mkl_service, numpy._core._multiarray_umath, numpy._core._multiarray_tests, numpy.linalg._umath_linalg, pyarrow.lib, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pyarrow._compute, pandas._libs.ops, pandas._libs.hashing, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.internals, pandas._libs.indexing, pandas._libs.index, pandas._libs.writers, pandas._libs.join, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, pyarrow._parquet, pyarrow._json, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, gmpy2.gmpy2, scipy._lib._ccallback_c, scipy.signal._sigtools, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.special._ufuncs_cxx, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.special._ellip_harm_2, scipy._lib._uarray._uarray, scipy.signal._max_len_seq_inner, scipy.signal._upfirdn_apply, scipy.signal._spline, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._distance_wrap, scipy.spatial._hausdorff, scipy.spatial.transform._rotation, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._cobyla, scipy.optimize._slsqp, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy.optimize._cython_nnls, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.interpolate._bspl, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, scipy.signal._sosfilt, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.special.cython_special, scipy.stats._stats, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.stats._mvn, scipy.stats._rcont.rcont, scipy.signal._peak_finding_utils, PIL._imaging, kiwisolver._cext, h5py._errors, h5py.defs, h5py._objects, h5py.h5, h5py.utils, h5py.h5t, h5py.h5s, h5py.h5ac, h5py.h5p, h5py.h5r, h5py._proxy, h5py._conv, h5py.h5z, h5py.h5a, h5py.h5d, h5py.h5ds, h5py.h5g, h5py.h5i, h5py.h5o, h5py.h5f, h5py.h5fd, h5py.h5pl, h5py.h5l, h5py._selector, sklearn.__check_build._check_build, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, sklearn.utils._random (total: 220)
[36m(RayTrainWorker pid=1721198)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-07_04-03-19/TorchTrainer_62632_00007_7_lr=0.0010,noise_std=0.1000,weight_decay=0.0000_2025-02-07_04-03-23/checkpoint_000099)[32m [repeated 7x across cluster][0m
