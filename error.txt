2025-03-11 21:21:29,449	INFO worker.py:1841 -- Started a local Ray instance.
2025-03-11 21:21:33,122	INFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
2025-03-11 21:21:33,154	INFO data_parallel_trainer.py:340 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.
2025-03-11 21:21:33,157	INFO data_parallel_trainer.py:340 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.
2025-03-11 21:21:33,159	INFO data_parallel_trainer.py:340 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.
2025-03-11 21:21:33,162	INFO data_parallel_trainer.py:340 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.
2025-03-11 21:21:33,164	INFO data_parallel_trainer.py:340 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.
2025-03-11 21:21:33,166	INFO data_parallel_trainer.py:340 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.
[36m(TorchTrainer pid=1566096)[0m GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.
[36m(RayTrainWorker pid=1567359)[0m Setting up process group for: env:// [rank=0, world_size=8]
[36m(TorchTrainer pid=1566098)[0m GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.[32m [repeated 5x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(RayTrainWorker pid=1567342)[0m Setting up process group for: env:// [rank=0, world_size=8]
[36m(TorchTrainer pid=1566097)[0m Started distributed worker processes: 
[36m(TorchTrainer pid=1566097)[0m - (node_id=ccb6df3744f8c871b5ea1a7a052b090b7fc6c0dabc8cc45e56a8c8cb, ip=172.17.6.108, pid=1567359) world_rank=0, local_rank=0, node_rank=0
[36m(TorchTrainer pid=1566097)[0m - (node_id=ccb6df3744f8c871b5ea1a7a052b090b7fc6c0dabc8cc45e56a8c8cb, ip=172.17.6.108, pid=1567352) world_rank=1, local_rank=1, node_rank=0
[36m(TorchTrainer pid=1566097)[0m - (node_id=ccb6df3744f8c871b5ea1a7a052b090b7fc6c0dabc8cc45e56a8c8cb, ip=172.17.6.108, pid=1567370) world_rank=2, local_rank=2, node_rank=0
[36m(TorchTrainer pid=1566097)[0m - (node_id=ccb6df3744f8c871b5ea1a7a052b090b7fc6c0dabc8cc45e56a8c8cb, ip=172.17.6.108, pid=1567348) world_rank=3, local_rank=3, node_rank=0
[36m(TorchTrainer pid=1566097)[0m - (node_id=ccb6df3744f8c871b5ea1a7a052b090b7fc6c0dabc8cc45e56a8c8cb, ip=172.17.6.108, pid=1567350) world_rank=4, local_rank=4, node_rank=0
[36m(TorchTrainer pid=1566097)[0m - (node_id=ccb6df3744f8c871b5ea1a7a052b090b7fc6c0dabc8cc45e56a8c8cb, ip=172.17.6.108, pid=1567349) world_rank=5, local_rank=5, node_rank=0
[36m(TorchTrainer pid=1566097)[0m - (node_id=ccb6df3744f8c871b5ea1a7a052b090b7fc6c0dabc8cc45e56a8c8cb, ip=172.17.6.108, pid=1567351) world_rank=6, local_rank=6, node_rank=0
[36m(TorchTrainer pid=1566097)[0m - (node_id=ccb6df3744f8c871b5ea1a7a052b090b7fc6c0dabc8cc45e56a8c8cb, ip=172.17.6.108, pid=1567387) world_rank=7, local_rank=7, node_rank=0
[36m(RayTrainWorker pid=1567391)[0m Setting up process group for: env:// [rank=0, world_size=8]
[36m(TorchTrainer pid=1566099)[0m Started distributed worker processes: 
[36m(TorchTrainer pid=1566099)[0m - (node_id=ccb6df3744f8c871b5ea1a7a052b090b7fc6c0dabc8cc45e56a8c8cb, ip=172.17.6.108, pid=1567342) world_rank=0, local_rank=0, node_rank=0
[36m(TorchTrainer pid=1566099)[0m - (node_id=ccb6df3744f8c871b5ea1a7a052b090b7fc6c0dabc8cc45e56a8c8cb, ip=172.17.6.108, pid=1567344) world_rank=1, local_rank=1, node_rank=0
[36m(TorchTrainer pid=1566099)[0m - (node_id=ccb6df3744f8c871b5ea1a7a052b090b7fc6c0dabc8cc45e56a8c8cb, ip=172.17.6.108, pid=1567367) world_rank=2, local_rank=2, node_rank=0
[36m(TorchTrainer pid=1566099)[0m - (node_id=ccb6df3744f8c871b5ea1a7a052b090b7fc6c0dabc8cc45e56a8c8cb, ip=172.17.6.108, pid=1567374) world_rank=3, local_rank=3, node_rank=0
[36m(TorchTrainer pid=1566099)[0m - (node_id=ccb6df3744f8c871b5ea1a7a052b090b7fc6c0dabc8cc45e56a8c8cb, ip=172.17.6.108, pid=1567396) world_rank=4, local_rank=4, node_rank=0
[36m(TorchTrainer pid=1566099)[0m - (node_id=ccb6df3744f8c871b5ea1a7a052b090b7fc6c0dabc8cc45e56a8c8cb, ip=172.17.6.108, pid=1567393) world_rank=5, local_rank=5, node_rank=0
[36m(TorchTrainer pid=1566099)[0m - (node_id=ccb6df3744f8c871b5ea1a7a052b090b7fc6c0dabc8cc45e56a8c8cb, ip=172.17.6.108, pid=1567389) world_rank=6, local_rank=6, node_rank=0
[36m(TorchTrainer pid=1566099)[0m - (node_id=ccb6df3744f8c871b5ea1a7a052b090b7fc6c0dabc8cc45e56a8c8cb, ip=172.17.6.108, pid=1567375) world_rank=7, local_rank=7, node_rank=0
[36m(RayTrainWorker pid=1567359)[0m GPU available: False, used: False
[36m(RayTrainWorker pid=1567359)[0m TPU available: False, using: 0 TPU cores
[36m(RayTrainWorker pid=1567359)[0m HPU available: False, using: 0 HPUs
[36m(RayTrainWorker pid=1567359)[0m /home/ad2002/.conda/envs/neuralnets/lib/python3.12/site-packages/lightning/pytorch/loops/utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
[36m(RayTrainWorker pid=1567354)[0m Setting up process group for: env:// [rank=0, world_size=8][32m [repeated 3x across cluster][0m
[36m(RayTrainWorker pid=1567359)[0m 
[36m(RayTrainWorker pid=1567359)[0m   | Name  | Type  | Params | Mode 
[36m(RayTrainWorker pid=1567359)[0m ----------------------------------------
[36m(RayTrainWorker pid=1567359)[0m 0 | model | lrRNN | 1.3 K  | train
[36m(RayTrainWorker pid=1567359)[0m ----------------------------------------
[36m(RayTrainWorker pid=1567359)[0m 516       Trainable params
[36m(RayTrainWorker pid=1567359)[0m 768       Non-trainable params
[36m(RayTrainWorker pid=1567359)[0m 1.3 K     Total params
[36m(RayTrainWorker pid=1567359)[0m 0.005     Total estimated model params size (MB)
[36m(RayTrainWorker pid=1567359)[0m 1         Modules in train mode
[36m(RayTrainWorker pid=1567359)[0m 0         Modules in eval mode
[36m(RayTrainWorker pid=1567342)[0m 
[36m(RayTrainWorker pid=1567365)[0m 
[36m(TorchTrainer pid=1566098)[0m Started distributed worker processes: [32m [repeated 4x across cluster][0m
[36m(TorchTrainer pid=1566098)[0m - (node_id=ccb6df3744f8c871b5ea1a7a052b090b7fc6c0dabc8cc45e56a8c8cb, ip=172.17.6.108, pid=1567399) world_rank=7, local_rank=7, node_rank=0[32m [repeated 32x across cluster][0m
[36m(RayTrainWorker pid=1567365)[0m GPU available: False, used: False[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1567365)[0m TPU available: False, using: 0 TPU cores[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1567365)[0m HPU available: False, using: 0 HPUs[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1567365)[0m /home/ad2002/.conda/envs/neuralnets/lib/python3.12/site-packages/lightning/pytorch/loops/utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1567365)[0m   | Name  | Type  | Params | Mode [32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1567365)[0m ----------------------------------------[32m [repeated 4x across cluster][0m
[36m(RayTrainWorker pid=1567365)[0m 0 | model | lrRNN | 1.3 K  | train[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1567365)[0m 516       Trainable params[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1567365)[0m 768       Non-trainable params[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1567365)[0m 1.3 K     Total params[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1567365)[0m 0.005     Total estimated model params size (MB)[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1567365)[0m 1         Modules in train mode[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1567365)[0m 0         Modules in eval mode[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1567391)[0m 
[36m(RayTrainWorker pid=1567368)[0m 
[36m(RayTrainWorker pid=1567354)[0m 
[36m(RayTrainWorker pid=1567359)[0m /home/ad2002/.conda/envs/neuralnets/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[36m(RayTrainWorker pid=1567354)[0m GPU available: False, used: False[32m [repeated 3x across cluster][0m
[36m(RayTrainWorker pid=1567354)[0m TPU available: False, using: 0 TPU cores[32m [repeated 3x across cluster][0m
[36m(RayTrainWorker pid=1567354)[0m HPU available: False, using: 0 HPUs[32m [repeated 3x across cluster][0m
[36m(RayTrainWorker pid=1567354)[0m /home/ad2002/.conda/envs/neuralnets/lib/python3.12/site-packages/lightning/pytorch/loops/utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.[32m [repeated 3x across cluster][0m
[36m(RayTrainWorker pid=1567354)[0m   | Name  | Type  | Params | Mode [32m [repeated 3x across cluster][0m
[36m(RayTrainWorker pid=1567354)[0m ----------------------------------------[32m [repeated 6x across cluster][0m
[36m(RayTrainWorker pid=1567354)[0m 0 | model | lrRNN | 1.3 K  | train[32m [repeated 3x across cluster][0m
[36m(RayTrainWorker pid=1567354)[0m 516       Trainable params[32m [repeated 3x across cluster][0m
[36m(RayTrainWorker pid=1567354)[0m 768       Non-trainable params[32m [repeated 3x across cluster][0m
[36m(RayTrainWorker pid=1567354)[0m 1.3 K     Total params[32m [repeated 3x across cluster][0m
[36m(RayTrainWorker pid=1567354)[0m 0.005     Total estimated model params size (MB)[32m [repeated 3x across cluster][0m
[36m(RayTrainWorker pid=1567354)[0m 1         Modules in train mode[32m [repeated 3x across cluster][0m
[36m(RayTrainWorker pid=1567354)[0m 0         Modules in eval mode[32m [repeated 3x across cluster][0m
[36m(RayTrainWorker pid=1567368)[0m /home/ad2002/.conda/envs/neuralnets/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.[32m [repeated 4x across cluster][0m
[36m(RayTrainWorker pid=1567359)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-03-11_21-21-28/TorchTrainer_556f4_00002_2_lr=0.0000,num_layers=3,weight_decay=0.0000_2025-03-11_21-21-33/checkpoint_000000)
[36m(RayTrainWorker pid=1567354)[0m /home/ad2002/.conda/envs/neuralnets/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
[36m(RayTrainWorker pid=1567368)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-03-11_21-21-28/TorchTrainer_556f4_00003_3_lr=0.0010,num_layers=10,weight_decay=0.0000_2025-03-11_21-21-33/checkpoint_000000)[32m [repeated 24x across cluster][0m
[36m(RayTrainWorker pid=1567383)[0m *** SIGSEGV received at time=1741742564 on cpu 36 ***
[36m(RayTrainWorker pid=1567383)[0m PC: @     0x14b6fac6c2fe  (unknown)  ray::gcs::TaskInfoAccessor::AsyncAddTaskEventData()
[36m(RayTrainWorker pid=1567383)[0m     @     0x14b6fc961990       3392  (unknown)
[36m(RayTrainWorker pid=1567383)[0m     @     0x14b6fab4f045       1392  ray::core::worker::TaskEventBufferImpl::FlushEvents()
[36m(RayTrainWorker pid=1567383)[0m     @     0x14b6faadb00c       1488  ray::core::CoreWorker::Disconnect()
[36m(RayTrainWorker pid=1567383)[0m     @     0x14b6faadb3bd       1152  ray::core::CoreWorker::ForceExit()
[36m(RayTrainWorker pid=1567383)[0m     @     0x14b6faadb7ef       1680  ray::core::CoreWorker::HandleKillActor()
[36m(RayTrainWorker pid=1567383)[0m     @     0x14b6faad2cf4        192  ray::rpc::ServerCallImpl<>::HandleRequestImpl()
[36m(RayTrainWorker pid=1567383)[0m     @     0x14b6fae21728       1168  EventTracker::RecordExecution()
[36m(RayTrainWorker pid=1567383)[0m     @     0x14b6fae1c6fe         48  std::_Function_handler<>::_M_invoke()
[36m(RayTrainWorker pid=1567383)[0m     @     0x14b6fae1cb76        112  boost::asio::detail::completion_handler<>::do_complete()
[36m(RayTrainWorker pid=1567383)[0m     @     0x14b6fb3f22bb        128  boost::asio::detail::scheduler::do_run_one()
[36m(RayTrainWorker pid=1567383)[0m     @     0x14b6fb3f3c39        288  boost::asio::detail::scheduler::run()
[36m(RayTrainWorker pid=1567383)[0m     @     0x14b6fb3f4342         96  boost::asio::io_context::run()
[36m(RayTrainWorker pid=1567383)[0m     @     0x14b6faa18101       1280  ray::core::CoreWorker::RunIOService()
[36m(RayTrainWorker pid=1567383)[0m     @     0x14b6faed2e90         64  thread_proxy
[36m(RayTrainWorker pid=1567383)[0m     @     0x14b6fc9571ca  (unknown)  start_thread
[36m(RayTrainWorker pid=1567383)[0m [2025-03-11 21:22:44,823 E 1567383 1570307] logging.cc:460: *** SIGSEGV received at time=1741742564 on cpu 36 ***
[36m(RayTrainWorker pid=1567383)[0m [2025-03-11 21:22:44,823 E 1567383 1570307] logging.cc:460: PC: @     0x14b6fac6c2fe  (unknown)  ray::gcs::TaskInfoAccessor::AsyncAddTaskEventData()
[36m(RayTrainWorker pid=1567383)[0m [2025-03-11 21:22:44,823 E 1567383 1570307] logging.cc:460:     @     0x14b6fc961990       3392  (unknown)
[36m(RayTrainWorker pid=1567383)[0m [2025-03-11 21:22:44,823 E 1567383 1570307] logging.cc:460:     @     0x14b6fab4f045       1392  ray::core::worker::TaskEventBufferImpl::FlushEvents()
[36m(RayTrainWorker pid=1567383)[0m [2025-03-11 21:22:44,823 E 1567383 1570307] logging.cc:460:     @     0x14b6faadb00c       1488  ray::core::CoreWorker::Disconnect()
[36m(RayTrainWorker pid=1567383)[0m [2025-03-11 21:22:44,823 E 1567383 1570307] logging.cc:460:     @     0x14b6faadb3bd       1152  ray::core::CoreWorker::ForceExit()
[36m(RayTrainWorker pid=1567383)[0m [2025-03-11 21:22:44,823 E 1567383 1570307] logging.cc:460:     @     0x14b6faadb7ef       1680  ray::core::CoreWorker::HandleKillActor()
[36m(RayTrainWorker pid=1567383)[0m [2025-03-11 21:22:44,823 E 1567383 1570307] logging.cc:460:     @     0x14b6faad2cf4        192  ray::rpc::ServerCallImpl<>::HandleRequestImpl()
[36m(RayTrainWorker pid=1567383)[0m [2025-03-11 21:22:44,823 E 1567383 1570307] logging.cc:460:     @     0x14b6fae21728       1168  EventTracker::RecordExecution()
[36m(RayTrainWorker pid=1567383)[0m [2025-03-11 21:22:44,823 E 1567383 1570307] logging.cc:460:     @     0x14b6fae1c6fe         48  std::_Function_handler<>::_M_invoke()
[36m(RayTrainWorker pid=1567383)[0m [2025-03-11 21:22:44,823 E 1567383 1570307] logging.cc:460:     @     0x14b6fae1cb76        112  boost::asio::detail::completion_handler<>::do_complete()
[36m(RayTrainWorker pid=1567383)[0m [2025-03-11 21:22:44,823 E 1567383 1570307] logging.cc:460:     @     0x14b6fb3f22bb        128  boost::asio::detail::scheduler::do_run_one()
[36m(RayTrainWorker pid=1567383)[0m [2025-03-11 21:22:44,823 E 1567383 1570307] logging.cc:460:     @     0x14b6fb3f3c39        288  boost::asio::detail::scheduler::run()
[36m(RayTrainWorker pid=1567383)[0m [2025-03-11 21:22:44,823 E 1567383 1570307] logging.cc:460:     @     0x14b6fb3f4342         96  boost::asio::io_context::run()
[36m(RayTrainWorker pid=1567383)[0m [2025-03-11 21:22:44,823 E 1567383 1570307] logging.cc:460:     @     0x14b6faa18101       1280  ray::core::CoreWorker::RunIOService()
[36m(RayTrainWorker pid=1567383)[0m [2025-03-11 21:22:44,823 E 1567383 1570307] logging.cc:460:     @     0x14b6faed2e90         64  thread_proxy
[36m(RayTrainWorker pid=1567383)[0m [2025-03-11 21:22:44,823 E 1567383 1570307] logging.cc:460:     @     0x14b6fc9571ca  (unknown)  start_thread
[36m(RayTrainWorker pid=1567383)[0m Fatal Python error: Segmentation fault
[36m(RayTrainWorker pid=1567383)[0m 
[36m(RayTrainWorker pid=1567383)[0m 
[36m(RayTrainWorker pid=1567383)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, ray._raylet, mkl._mklinit, mkl._py_mkl_service, numpy._core._multiarray_umath, numpy._core._multiarray_tests, numpy.linalg._umath_linalg, pyarrow.lib, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pyarrow._compute, pandas._libs.ops, pandas._libs.hashing, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.internals, pandas._libs.indexing, pandas._libs.index, pandas._libs.writers, pandas._libs.join, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, pyarrow._parquet, pyarrow._json, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, gmpy2.gmpy2, scipy._lib._ccallback_c, scipy.signal._sigtools, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.special._ufuncs_cxx, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.special._ellip_harm_2, scipy._lib._uarray._uarray, scipy.signal._max_len_seq_inner, scipy.signal._upfirdn_apply, scipy.signal._spline, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._distance_wrap, scipy.spatial._hausdorff, scipy.spatial.transform._rotation, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._cobyla, scipy.optimize._slsqp, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy.optimize._cython_nnls, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.interpolate._bspl, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, scipy.signal._sosfilt, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.special.cython_special, scipy.stats._stats, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.stats._mvn, scipy.stats._rcont.rcont, scipy.signal._peak_finding_utils, PIL._imaging, kiwisolver._cext, h5py._errors, h5py.defs, h5py._objects, h5py.h5, h5py.utils, h5py.h5t, h5py.h5s, h5py.h5ac, h5py.h5p, h5py.h5r, h5py._proxy, h5py._conv, h5py.h5z, h5py.h5a, h5py.h5d, h5py.h5ds, h5py.h5g, h5py.h5i, h5py.h5o, h5py.h5f, h5py.h5fd, h5py.h5pl, h5py.h5l, h5py._selector, sklearn.__check_build._check_build, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, sklearn.utils._random (total: 220)
[36m(RayTrainWorker pid=1567365)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-03-11_21-21-28/TorchTrainer_556f4_00001_1_lr=0.0001,num_layers=10,weight_decay=0.0010_2025-03-11_21-21-33/checkpoint_000001)[32m [repeated 24x across cluster][0m
[36m(RayTrainWorker pid=1567375)[0m *** SIGSEGV received at time=1741742579 on cpu 35 ***
[36m(RayTrainWorker pid=1567375)[0m PC: @     0x150cc9c0e2fe  (unknown)  ray::gcs::TaskInfoAccessor::AsyncAddTaskEventData()
[36m(RayTrainWorker pid=1567375)[0m     @     0x150ccb903990       3392  (unknown)
[36m(RayTrainWorker pid=1567375)[0m     @     0x150cc9af1045       1392  ray::core::worker::TaskEventBufferImpl::FlushEvents()
[36m(RayTrainWorker pid=1567375)[0m     @     0x150cc9a7d00c       1488  ray::core::CoreWorker::Disconnect()
[36m(RayTrainWorker pid=1567375)[0m     @     0x150cc9a7d3bd       1152  ray::core::CoreWorker::ForceExit()
[36m(RayTrainWorker pid=1567375)[0m     @     0x150cc9a7d7ef       1680  ray::core::CoreWorker::HandleKillActor()
[36m(RayTrainWorker pid=1567375)[0m     @     0x150cc9a74cf4        192  ray::rpc::ServerCallImpl<>::HandleRequestImpl()
[36m(RayTrainWorker pid=1567375)[0m     @     0x150cc9dc3728       1168  EventTracker::RecordExecution()
[36m(RayTrainWorker pid=1567375)[0m     @     0x150cc9dbe6fe         48  std::_Function_handler<>::_M_invoke()
[36m(RayTrainWorker pid=1567375)[0m     @     0x150cc9dbeb76        112  boost::asio::detail::completion_handler<>::do_complete()
[36m(RayTrainWorker pid=1567375)[0m     @     0x150cca3942bb        128  boost::asio::detail::scheduler::do_run_one()
[36m(RayTrainWorker pid=1567375)[0m     @     0x150cca395c39        288  boost::asio::detail::scheduler::run()
[36m(RayTrainWorker pid=1567375)[0m     @     0x150cca396342         96  boost::asio::io_context::run()
[36m(RayTrainWorker pid=1567375)[0m     @     0x150cc99ba101       1280  ray::core::CoreWorker::RunIOService()
[36m(RayTrainWorker pid=1567375)[0m     @     0x150cc9e74e90         64  thread_proxy
[36m(RayTrainWorker pid=1567375)[0m     @     0x150ccb8f91ca  (unknown)  start_thread
[36m(RayTrainWorker pid=1567375)[0m [2025-03-11 21:22:59,902 E 1567375 1569985] logging.cc:460: *** SIGSEGV received at time=1741742579 on cpu 35 ***
[36m(RayTrainWorker pid=1567375)[0m [2025-03-11 21:22:59,902 E 1567375 1569985] logging.cc:460: PC: @     0x150cc9c0e2fe  (unknown)  ray::gcs::TaskInfoAccessor::AsyncAddTaskEventData()
[36m(RayTrainWorker pid=1567375)[0m [2025-03-11 21:22:59,902 E 1567375 1569985] logging.cc:460:     @     0x150ccb903990       3392  (unknown)
[36m(RayTrainWorker pid=1567375)[0m [2025-03-11 21:22:59,902 E 1567375 1569985] logging.cc:460:     @     0x150cc9af1045       1392  ray::core::worker::TaskEventBufferImpl::FlushEvents()
[36m(RayTrainWorker pid=1567375)[0m [2025-03-11 21:22:59,902 E 1567375 1569985] logging.cc:460:     @     0x150cc9a7d00c       1488  ray::core::CoreWorker::Disconnect()
[36m(RayTrainWorker pid=1567375)[0m [2025-03-11 21:22:59,902 E 1567375 1569985] logging.cc:460:     @     0x150cc9a7d3bd       1152  ray::core::CoreWorker::ForceExit()
[36m(RayTrainWorker pid=1567375)[0m [2025-03-11 21:22:59,902 E 1567375 1569985] logging.cc:460:     @     0x150cc9a7d7ef       1680  ray::core::CoreWorker::HandleKillActor()
[36m(RayTrainWorker pid=1567375)[0m [2025-03-11 21:22:59,902 E 1567375 1569985] logging.cc:460:     @     0x150cc9a74cf4        192  ray::rpc::ServerCallImpl<>::HandleRequestImpl()
[36m(RayTrainWorker pid=1567375)[0m [2025-03-11 21:22:59,902 E 1567375 1569985] logging.cc:460:     @     0x150cc9dc3728       1168  EventTracker::RecordExecution()
[36m(RayTrainWorker pid=1567375)[0m [2025-03-11 21:22:59,902 E 1567375 1569985] logging.cc:460:     @     0x150cc9dbe6fe         48  std::_Function_handler<>::_M_invoke()
[36m(RayTrainWorker pid=1567375)[0m [2025-03-11 21:22:59,902 E 1567375 1569985] logging.cc:460:     @     0x150cc9dbeb76        112  boost::asio::detail::completion_handler<>::do_complete()
[36m(RayTrainWorker pid=1567375)[0m [2025-03-11 21:22:59,902 E 1567375 1569985] logging.cc:460:     @     0x150cca3942bb        128  boost::asio::detail::scheduler::do_run_one()
[36m(RayTrainWorker pid=1567375)[0m [2025-03-11 21:22:59,902 E 1567375 1569985] logging.cc:460:     @     0x150cca395c39        288  boost::asio::detail::scheduler::run()
[36m(RayTrainWorker pid=1567375)[0m [2025-03-11 21:22:59,902 E 1567375 1569985] logging.cc:460:     @     0x150cca396342         96  boost::asio::io_context::run()
[36m(RayTrainWorker pid=1567375)[0m [2025-03-11 21:22:59,902 E 1567375 1569985] logging.cc:460:     @     0x150cc99ba101       1280  ray::core::CoreWorker::RunIOService()
[36m(RayTrainWorker pid=1567375)[0m [2025-03-11 21:22:59,902 E 1567375 1569985] logging.cc:460:     @     0x150cc9e74e90         64  thread_proxy
[36m(RayTrainWorker pid=1567375)[0m [2025-03-11 21:22:59,902 E 1567375 1569985] logging.cc:460:     @     0x150ccb8f91ca  (unknown)  start_thread
[36m(RayTrainWorker pid=1567375)[0m Fatal Python error: Segmentation fault
[36m(RayTrainWorker pid=1567375)[0m 
[36m(RayTrainWorker pid=1567375)[0m 
[36m(RayTrainWorker pid=1567375)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, ray._raylet, mkl._mklinit, mkl._py_mkl_service, numpy._core._multiarray_umath, numpy._core._multiarray_tests, numpy.linalg._umath_linalg, pyarrow.lib, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pyarrow._compute, pandas._libs.ops, pandas._libs.hashing, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.internals, pandas._libs.indexing, pandas._libs.index, pandas._libs.writers, pandas._libs.join, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, pyarrow._parquet, pyarrow._json, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, gmpy2.gmpy2, scipy._lib._ccallback_c, scipy.signal._sigtools, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.special._ufuncs_cxx, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.special._ellip_harm_2, scipy._lib._uarray._uarray, scipy.signal._max_len_seq_inner, scipy.signal._upfirdn_apply, scipy.signal._spline, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._distance_wrap, scipy.spatial._hausdorff, scipy.spatial.transform._rotation, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._cobyla, scipy.optimize._slsqp, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy.optimize._cython_nnls, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.interpolate._bspl, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, scipy.signal._sosfilt, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.special.cython_special, scipy.stats._stats, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.stats._mvn, scipy.stats._rcont.rcont, scipy.signal._peak_finding_utils, PIL._imaging, kiwisolver._cext, h5py._errors, h5py.defs, h5py._objects, h5py.h5, h5py.utils, h5py.h5t, h5py.h5s, h5py.h5ac, h5py.h5p, h5py.h5r, h5py._proxy, h5py._conv, h5py.h5z, h5py.h5a, h5py.h5d, h5py.h5ds, h5py.h5g, h5py.h5i, h5py.h5o, h5py.h5f, h5py.h5fd, h5py.h5pl, h5py.h5l, h5py._selector, sklearn.__check_build._check_build, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, sklearn.utils._random (total: 220)
[36m(RayTrainWorker pid=1567359)[0m 
[36m(RayTrainWorker pid=1567359)[0m 
[36m(RayTrainWorker pid=1567356)[0m 
[36m(RayTrainWorker pid=1567356)[0m 
[36m(RayTrainWorker pid=1567365)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-03-11_21-21-28/TorchTrainer_556f4_00001_1_lr=0.0001,num_layers=10,weight_decay=0.0010_2025-03-11_21-21-33/checkpoint_000002)[32m [repeated 40x across cluster][0m
[36m(RayTrainWorker pid=1567356)[0m *** SIGSEGV received at time=1741742582 on cpu 38 ***[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1567356)[0m PC: @     0x14c791eb52fe  (unknown)  ray::gcs::TaskInfoAccessor::AsyncAddTaskEventData()[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1567356)[0m     @     0x14c793baa990       3392  (unknown)[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1567356)[0m     @     0x14c791d98045       1392  ray::core::worker::TaskEventBufferImpl::FlushEvents()[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1567356)[0m     @     0x14c791d2400c       1488  ray::core::CoreWorker::Disconnect()[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1567356)[0m     @     0x14c791d243bd       1152  ray::core::CoreWorker::ForceExit()[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1567356)[0m     @     0x14c791d247ef       1680  ray::core::CoreWorker::HandleKillActor()[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1567356)[0m     @     0x14c791d1bcf4        192  ray::rpc::ServerCallImpl<>::HandleRequestImpl()[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1567356)[0m     @     0x14c79206a728       1168  EventTracker::RecordExecution()[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1567356)[0m     @     0x14c7920656fe         48  std::_Function_handler<>::_M_invoke()[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1567356)[0m     @     0x14c792065b76        112  boost::asio::detail::completion_handler<>::do_complete()[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1567356)[0m     @     0x14c79263b2bb        128  boost::asio::detail::scheduler::do_run_one()[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1567356)[0m     @     0x14c79263cc39        288  boost::asio::detail::scheduler::run()[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1567356)[0m     @     0x14c79263d342         96  boost::asio::io_context::run()[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1567356)[0m     @     0x14c791c61101       1280  ray::core::CoreWorker::RunIOService()[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1567356)[0m     @     0x14c79211be90         64  thread_proxy[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1567356)[0m     @     0x14c793ba01ca  (unknown)  start_thread[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1567356)[0m [2025-03-11 21:23:02,845 E 1567356 1568761] logging.cc:460: *** SIGSEGV received at time=1741742582 on cpu 38 ***[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1567356)[0m [2025-03-11 21:23:02,845 E 1567356 1568761] logging.cc:460: PC: @     0x14c791eb52fe  (unknown)  ray::gcs::TaskInfoAccessor::AsyncAddTaskEventData()[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1567356)[0m [2025-03-11 21:23:02,845 E 1567356 1568761] logging.cc:460:     @     0x14c793baa990       3392  (unknown)[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1567356)[0m [2025-03-11 21:23:02,845 E 1567356 1568761] logging.cc:460:     @     0x14c791d98045       1392  ray::core::worker::TaskEventBufferImpl::FlushEvents()[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1567356)[0m [2025-03-11 21:23:02,845 E 1567356 1568761] logging.cc:460:     @     0x14c791d2400c       1488  ray::core::CoreWorker::Disconnect()[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1567356)[0m [2025-03-11 21:23:02,845 E 1567356 1568761] logging.cc:460:     @     0x14c791d243bd       1152  ray::core::CoreWorker::ForceExit()[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1567356)[0m [2025-03-11 21:23:02,845 E 1567356 1568761] logging.cc:460:     @     0x14c791d247ef       1680  ray::core::CoreWorker::HandleKillActor()[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1567356)[0m [2025-03-11 21:23:02,845 E 1567356 1568761] logging.cc:460:     @     0x14c791d1bcf4        192  ray::rpc::ServerCallImpl<>::HandleRequestImpl()[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1567356)[0m [2025-03-11 21:23:02,845 E 1567356 1568761] logging.cc:460:     @     0x14c79206a728       1168  EventTracker::RecordExecution()[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1567356)[0m [2025-03-11 21:23:02,845 E 1567356 1568761] logging.cc:460:     @     0x14c7920656fe         48  std::_Function_handler<>::_M_invoke()[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1567356)[0m [2025-03-11 21:23:02,845 E 1567356 1568761] logging.cc:460:     @     0x14c792065b76        112  boost::asio::detail::completion_handler<>::do_complete()[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1567356)[0m [2025-03-11 21:23:02,845 E 1567356 1568761] logging.cc:460:     @     0x14c79263b2bb        128  boost::asio::detail::scheduler::do_run_one()[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1567356)[0m [2025-03-11 21:23:02,845 E 1567356 1568761] logging.cc:460:     @     0x14c79263cc39        288  boost::asio::detail::scheduler::run()[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1567356)[0m [2025-03-11 21:23:02,845 E 1567356 1568761] logging.cc:460:     @     0x14c79263d342         96  boost::asio::io_context::run()[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1567356)[0m [2025-03-11 21:23:02,845 E 1567356 1568761] logging.cc:460:     @     0x14c791c61101       1280  ray::core::CoreWorker::RunIOService()[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1567356)[0m [2025-03-11 21:23:02,845 E 1567356 1568761] logging.cc:460:     @     0x14c79211be90         64  thread_proxy[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1567356)[0m [2025-03-11 21:23:02,845 E 1567356 1568761] logging.cc:460:     @     0x14c793ba01ca  (unknown)  start_thread[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1567356)[0m Fatal Python error: Segmentation fault[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1567356)[0m Extension modules: msgpack._cmsgpack, google._upb._message, psutil._psutil_linux, psutil._psutil_posix, setproctitle, yaml._yaml, _brotli, ray._raylet, mkl._mklinit, mkl._py_mkl_service, numpy._core._multiarray_umath, numpy._core._multiarray_tests, numpy.linalg._umath_linalg, pyarrow.lib, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pyarrow._compute, pandas._libs.ops, pandas._libs.hashing, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.internals, pandas._libs.indexing, pandas._libs.index, pandas._libs.writers, pandas._libs.join, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs, pyarrow._gcsfs, pyarrow._s3fs, pyarrow._parquet, pyarrow._json, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, gmpy2.gmpy2, scipy._lib._ccallback_c, scipy.signal._sigtools, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.special._ufuncs_cxx, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.special._ellip_harm_2, scipy._lib._uarray._uarray, scipy.signal._max_len_seq_inner, scipy.signal._upfirdn_apply, scipy.signal._spline, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._distance_wrap, scipy.spatial._hausdorff, scipy.spatial.transform._rotation, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._cobyla, scipy.optimize._slsqp, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy.optimize._cython_nnls, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.interpolate._bspl, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, scipy.signal._sosfilt, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.special.cython_special, scipy.stats._stats, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.stats._mvn, scipy.stats._rcont.rcont, scipy.signal._peak_finding_utils, PIL._imaging, kiwisolver._cext, h5py._errors, h5py.defs, h5py._objects, h5py.h5, h5py.utils, h5py.h5t, h5py.h5s, h5py.h5ac, h5py.h5p, h5py.h5r, h5py._proxy, h5py._conv, h5py.h5z, h5py.h5a, h5py.h5d, h5py.h5ds, h5py.h5g, h5py.h5i, h5py.h5o, h5py.h5f, h5py.h5fd, h5py.h5pl, h5py.h5l, h5py._selector, sklearn.__check_build._check_build, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, sklearn.utils._random (total: 220)[32m [repeated 2x across cluster][0m
[36m(RayTrainWorker pid=1567365)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-03-11_21-21-28/TorchTrainer_556f4_00001_1_lr=0.0001,num_layers=10,weight_decay=0.0010_2025-03-11_21-21-33/checkpoint_000003)[32m [repeated 16x across cluster][0m
[36m(RayTrainWorker pid=1567365)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-03-11_21-21-28/TorchTrainer_556f4_00001_1_lr=0.0001,num_layers=10,weight_decay=0.0010_2025-03-11_21-21-33/checkpoint_000004)[32m [repeated 16x across cluster][0m
[36m(RayTrainWorker pid=1567354)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-03-11_21-21-28/TorchTrainer_556f4_00005_5_lr=0.0001,num_layers=3,weight_decay=0.0010_2025-03-11_21-21-33/checkpoint_000004)[32m [repeated 8x across cluster][0m
[36m(RayTrainWorker pid=1567354)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-03-11_21-21-28/TorchTrainer_556f4_00005_5_lr=0.0001,num_layers=3,weight_decay=0.0010_2025-03-11_21-21-33/checkpoint_000005)[32m [repeated 16x across cluster][0m
[36m(RayTrainWorker pid=1567365)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-03-11_21-21-28/TorchTrainer_556f4_00001_1_lr=0.0001,num_layers=10,weight_decay=0.0010_2025-03-11_21-21-33/checkpoint_000007)[32m [repeated 16x across cluster][0m
[36m(RayTrainWorker pid=1567365)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-03-11_21-21-28/TorchTrainer_556f4_00001_1_lr=0.0001,num_layers=10,weight_decay=0.0010_2025-03-11_21-21-33/checkpoint_000008)[32m [repeated 16x across cluster][0m
[36m(RayTrainWorker pid=1567365)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-03-11_21-21-28/TorchTrainer_556f4_00001_1_lr=0.0001,num_layers=10,weight_decay=0.0010_2025-03-11_21-21-33/checkpoint_000009)[32m [repeated 16x across cluster][0m
[36m(RayTrainWorker pid=1567365)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-03-11_21-21-28/TorchTrainer_556f4_00001_1_lr=0.0001,num_layers=10,weight_decay=0.0010_2025-03-11_21-21-33/checkpoint_000010)[32m [repeated 16x across cluster][0m
[36m(RayTrainWorker pid=1567354)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-03-11_21-21-28/TorchTrainer_556f4_00005_5_lr=0.0001,num_layers=3,weight_decay=0.0010_2025-03-11_21-21-33/checkpoint_000009)[32m [repeated 8x across cluster][0m
[36m(RayTrainWorker pid=1567354)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-03-11_21-21-28/TorchTrainer_556f4_00005_5_lr=0.0001,num_layers=3,weight_decay=0.0010_2025-03-11_21-21-33/checkpoint_000010)[32m [repeated 16x across cluster][0m
[36m(RayTrainWorker pid=1567365)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-03-11_21-21-28/TorchTrainer_556f4_00001_1_lr=0.0001,num_layers=10,weight_decay=0.0010_2025-03-11_21-21-33/checkpoint_000013)[32m [repeated 16x across cluster][0m
[36m(RayTrainWorker pid=1567365)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-03-11_21-21-28/TorchTrainer_556f4_00001_1_lr=0.0001,num_layers=10,weight_decay=0.0010_2025-03-11_21-21-33/checkpoint_000014)[32m [repeated 16x across cluster][0m
[36m(RayTrainWorker pid=1567365)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-03-11_21-21-28/TorchTrainer_556f4_00001_1_lr=0.0001,num_layers=10,weight_decay=0.0010_2025-03-11_21-21-33/checkpoint_000015)[32m [repeated 16x across cluster][0m
[36m(RayTrainWorker pid=1567365)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-03-11_21-21-28/TorchTrainer_556f4_00001_1_lr=0.0001,num_layers=10,weight_decay=0.0010_2025-03-11_21-21-33/checkpoint_000016)[32m [repeated 16x across cluster][0m
[36m(RayTrainWorker pid=1567365)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-03-11_21-21-28/TorchTrainer_556f4_00001_1_lr=0.0001,num_layers=10,weight_decay=0.0010_2025-03-11_21-21-33/checkpoint_000017)[32m [repeated 16x across cluster][0m
[36m(RayTrainWorker pid=1567365)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-03-11_21-21-28/TorchTrainer_556f4_00001_1_lr=0.0001,num_layers=10,weight_decay=0.0010_2025-03-11_21-21-33/checkpoint_000018)[32m [repeated 16x across cluster][0m
[36m(RayTrainWorker pid=1567354)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-03-11_21-21-28/TorchTrainer_556f4_00005_5_lr=0.0001,num_layers=3,weight_decay=0.0010_2025-03-11_21-21-33/checkpoint_000016)[32m [repeated 8x across cluster][0m
[36m(RayTrainWorker pid=1567390)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-03-11_21-21-28/TorchTrainer_556f4_00005_5_lr=0.0001,num_layers=3,weight_decay=0.0010_2025-03-11_21-21-33/checkpoint_000017)[32m [repeated 16x across cluster][0m
[36m(RayTrainWorker pid=1567365)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-03-11_21-21-28/TorchTrainer_556f4_00001_1_lr=0.0001,num_layers=10,weight_decay=0.0010_2025-03-11_21-21-33/checkpoint_000021)[32m [repeated 16x across cluster][0m
[36m(RayTrainWorker pid=1567365)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-03-11_21-21-28/TorchTrainer_556f4_00001_1_lr=0.0001,num_layers=10,weight_decay=0.0010_2025-03-11_21-21-33/checkpoint_000022)[32m [repeated 16x across cluster][0m
[36m(RayTrainWorker pid=1567365)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-03-11_21-21-28/TorchTrainer_556f4_00001_1_lr=0.0001,num_layers=10,weight_decay=0.0010_2025-03-11_21-21-33/checkpoint_000023)[32m [repeated 16x across cluster][0m
