
CondaError: Run 'conda init' before 'conda activate'

2025-02-01 14:53:59,381	INFO worker.py:1841 -- Started a local Ray instance.
2025-02-01 14:54:03,847	INFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.
[36m(RayTrainWorker pid=2697733)[0m Setting up process group for: env:// [rank=0, world_size=4]
[36m(TorchTrainer pid=2697546)[0m Started distributed worker processes: 
[36m(TorchTrainer pid=2697546)[0m - (node_id=a95697384a8d7ceb1e94027b5e0f9f6dbb44b1f8ef9f12ce87dca1e5, ip=172.17.6.68, pid=2697733) world_rank=0, local_rank=0, node_rank=0
[36m(TorchTrainer pid=2697546)[0m - (node_id=a95697384a8d7ceb1e94027b5e0f9f6dbb44b1f8ef9f12ce87dca1e5, ip=172.17.6.68, pid=2697732) world_rank=1, local_rank=1, node_rank=0
[36m(TorchTrainer pid=2697546)[0m - (node_id=a95697384a8d7ceb1e94027b5e0f9f6dbb44b1f8ef9f12ce87dca1e5, ip=172.17.6.68, pid=2697734) world_rank=2, local_rank=2, node_rank=0
[36m(TorchTrainer pid=2697546)[0m - (node_id=a95697384a8d7ceb1e94027b5e0f9f6dbb44b1f8ef9f12ce87dca1e5, ip=172.17.6.68, pid=2697731) world_rank=3, local_rank=3, node_rank=0
2025-02-01 14:54:14,027	ERROR tune_controller.py:1331 -- Trial task failed for trial TorchTrainer_49db4_00000
Traceback (most recent call last):
  File "/home/ad2002/.conda/envs/neuralnets/lib/python3.12/site-packages/ray/air/execution/_internal/event_manager.py", line 110, in resolve_future
    result = ray.get(future)
             ^^^^^^^^^^^^^^^
  File "/home/ad2002/.conda/envs/neuralnets/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/ad2002/.conda/envs/neuralnets/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ad2002/.conda/envs/neuralnets/lib/python3.12/site-packages/ray/_private/worker.py", line 2772, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ad2002/.conda/envs/neuralnets/lib/python3.12/site-packages/ray/_private/worker.py", line 919, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(BlockingIOError): [36mray::_Inner.train()[39m (pid=2697546, ip=172.17.6.68, actor_id=62965dc72c2de8894135de0501000000, repr=TorchTrainer)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ad2002/.conda/envs/neuralnets/lib/python3.12/site-packages/ray/tune/trainable/trainable.py", line 331, in train
    raise skipped from exception_cause(skipped)
  File "/home/ad2002/.conda/envs/neuralnets/lib/python3.12/site-packages/ray/train/_internal/utils.py", line 57, in check_for_failure
    ray.get(object_ref)
           ^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ray.exceptions.RayTaskError(BlockingIOError): [36mray::_RayTrainWorker__execute.get_next()[39m (pid=2697732, ip=172.17.6.68, actor_id=b0f0a584e97e666c635290b701000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x1515ce7eba10>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ad2002/.conda/envs/neuralnets/lib/python3.12/site-packages/ray/train/_internal/worker_group.py", line 33, in __execute
    raise skipped from exception_cause(skipped)
  File "/home/ad2002/.conda/envs/neuralnets/lib/python3.12/site-packages/ray/train/_internal/utils.py", line 176, in discard_return_wrapper
    train_func(*args, **kwargs)
  File "/home/ad2002/thesis/scripts_to_run/train_cluster.py", line 115, in train_loop
    data_module.prepare_data()
  File "/home/ad2002/thesis/synthetic_datasets/datamodules/task_datamodule.py", line 104, in prepare_data
    with h5py.File(self.dpath, 'w') as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ad2002/.conda/envs/neuralnets/lib/python3.12/site-packages/h5py/_hl/files.py", line 561, in __init__
    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ad2002/.conda/envs/neuralnets/lib/python3.12/site-packages/h5py/_hl/files.py", line 241, in make_fid
    fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "h5py/_objects.pyx", line 54, in h5py._objects.with_phil.wrapper
  File "h5py/_objects.pyx", line 55, in h5py._objects.with_phil.wrapper
  File "h5py/h5f.pyx", line 122, in h5py.h5f.create
BlockingIOError: [Errno 11] Unable to synchronously create file (unable to lock file, errno = 11, error message = 'Resource temporarily unavailable')
2025-02-01 14:54:14,035	INFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/scratch/gpfs/ad2002/task_training/ray_results/TorchTrainer_2025-02-01_14-53-57' in 0.0039s.
2025-02-01 14:54:14,040	ERROR tune.py:1037 -- Trials did not complete: [TorchTrainer_49db4_00000]
[36m(RayTrainWorker pid=2697733)[0m GPU available: True (cuda), used: True
[36m(RayTrainWorker pid=2697733)[0m TPU available: False, using: 0 TPU cores
[36m(RayTrainWorker pid=2697733)[0m HPU available: False, using: 0 HPUs
